{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Competitor NER & Sentiment Analysis - FIXED VERSION\n",
    "\n",
    "**Key Changes from Original:**\n",
    "1. Simplified NER to **single-label classification** (which competitor is tweet about)\n",
    "2. Added **data validation** at every step\n",
    "3. Separate handling for **multi-competitor extraction** using regex\n",
    "4. More robust **label creation** with debugging\n",
    "\n",
    "**Pipeline:**\n",
    "1. NER Model: Tweet → Primary Competitor (14-class classification)\n",
    "2. Multi-Competitor Extraction: Tweet → All mentioned competitors (regex-based)\n",
    "3. Sentiment Model: (Tweet, Competitor) → Sentiment (3-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q transformers datasets torch torchvision accelerate\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install -q sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AdamW, get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, f1_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and configure device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    gpu_memory_gb = gpu_props.total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_props.name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
    "    \n",
    "    # Adaptive batch size\n",
    "    if gpu_memory_gb >= 15:\n",
    "        BATCH_SIZE = 16\n",
    "        GRAD_ACCUM_STEPS = 2\n",
    "    else:\n",
    "        BATCH_SIZE = 8\n",
    "        GRAD_ACCUM_STEPS = 4\n",
    "else:\n",
    "    BATCH_SIZE = 4\n",
    "    GRAD_ACCUM_STEPS = 8\n",
    "\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "print(f\"\\nBatch size: {BATCH_SIZE}, Accumulation: {GRAD_ACCUM_STEPS}, Effective: {EFFECTIVE_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p '/content/drive/MyDrive/KFC_ML_Models'\n",
    "!mkdir -p '/content/results'\n",
    "\n",
    "MODEL_SAVE_DIR = '/content/drive/MyDrive/KFC_ML_Models'\n",
    "RESULTS_DIR = '/content/results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitor list (14 total)\n",
    "COMPETITORS = [\n",
    "    'Burger King', 'Deliveroo', \"Domino's\", 'Five Guys', 'Greggs',\n",
    "    'Just Eat', 'KFC', \"McDonald's\", \"Nando's\", \"Papa John's\",\n",
    "    'Pizza Hut', 'Pret a Manger', 'Taco Bell', 'Uber Eats'\n",
    "]\n",
    "\n",
    "SENTIMENT_MAP = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# Model configs\n",
    "NER_MODEL_NAME = 'bert-base-uncased'  # Changed to uncased for robustness\n",
    "SENTIMENT_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_SEQ_LENGTH = 128\n",
    "NER_LEARNING_RATE = 2e-5\n",
    "SENTIMENT_LEARNING_RATE = 2e-5\n",
    "NER_EPOCHS = 5\n",
    "SENTIMENT_EPOCHS = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  Competitors: {len(COMPETITORS)}\")\n",
    "print(f\"  NER Model: {NER_MODEL_NAME}\")\n",
    "print(f\"  Sentiment Model: {SENTIMENT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload CSV files\n",
    "from google.colab import files\n",
    "print(\"Please upload your CSV files:\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "df_large = pd.read_csv('KFC_social_data.xlsx - Sheet1.csv', low_memory=False)\n",
    "df_train_sample = pd.read_csv('KFC_training_sample.csv')\n",
    "df_test = pd.read_csv('KFC_test_sample.csv')\n",
    "df_test_pred = pd.read_csv('KFC_test_sample_for_prediction.csv')\n",
    "\n",
    "print(f\"\\n✓ Loaded:\")\n",
    "print(f\"  Large dataset: {len(df_large)} rows\")\n",
    "print(f\"  Training sample: {len(df_train_sample)} rows\")\n",
    "print(f\"  Test (with labels): {len(df_test)} rows\")\n",
    "print(f\"  Test (for prediction): {len(df_test_pred)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentiment(value):\n",
    "    \"\"\"Extract numeric sentiment (0, 1, 2)\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    \n",
    "    if isinstance(value, (int, float)):\n",
    "        if value in [0, 1, 2]:\n",
    "            return int(value)\n",
    "        return None\n",
    "    \n",
    "    value_str = str(value).strip().lower()\n",
    "    if value_str in ['0', 'negative']: return 0\n",
    "    elif value_str in ['1', 'neutral']: return 1\n",
    "    elif value_str in ['2', 'positive']: return 2\n",
    "    \n",
    "    match = re.match(r'^(\\d)', value_str)\n",
    "    if match:\n",
    "        digit = int(match.group(1))\n",
    "        if digit in [0, 1, 2]:\n",
    "            return digit\n",
    "    \n",
    "    return None\n",
    "\n",
    "def normalize_competitor_name(comp_str):\n",
    "    \"\"\"Normalize competitor names to match COMPETITORS list\"\"\"\n",
    "    if pd.isna(comp_str) or not comp_str:\n",
    "        return None\n",
    "    \n",
    "    comp_str = str(comp_str).strip()\n",
    "    \n",
    "    # Direct match\n",
    "    if comp_str in COMPETITORS:\n",
    "        return comp_str\n",
    "    \n",
    "    # Case-insensitive match\n",
    "    for comp in COMPETITORS:\n",
    "        if comp.lower() == comp_str.lower():\n",
    "            return comp\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"✓ Data cleaning functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, name=\"dataset\"):\n",
    "    \"\"\"Clean and prepare dataset with validation\"\"\"\n",
    "    print(f\"\\nPreparing {name}...\")\n",
    "    print(f\"  Initial rows: {len(df)}\")\n",
    "    \n",
    "    # Select columns\n",
    "    essential_cols = ['Competitor', 'Tweet', 'SENTIMENT']\n",
    "    if 'Tweet' not in df.columns:\n",
    "        if 'Full Text' in df.columns:\n",
    "            df['Tweet'] = df['Full Text']\n",
    "        elif 'Snippet' in df.columns:\n",
    "            df['Tweet'] = df['Snippet']\n",
    "    \n",
    "    available_cols = [col for col in essential_cols if col in df.columns]\n",
    "    metadata_cols = [col for col in ['Impact', 'Impressions', 'Reach (new)'] if col in df.columns]\n",
    "    \n",
    "    df_clean = df[available_cols + metadata_cols].copy()\n",
    "    \n",
    "    # Clean sentiment\n",
    "    if 'SENTIMENT' in df_clean.columns:\n",
    "        df_clean['SENTIMENT'] = df_clean['SENTIMENT'].apply(clean_sentiment)\n",
    "        before = len(df_clean)\n",
    "        df_clean = df_clean.dropna(subset=['SENTIMENT'])\n",
    "        print(f\"  Dropped {before - len(df_clean)} rows with invalid sentiment\")\n",
    "        df_clean['SENTIMENT'] = df_clean['SENTIMENT'].astype(int)\n",
    "    \n",
    "    # Clean competitor names\n",
    "    df_clean['Competitor'] = df_clean['Competitor'].apply(normalize_competitor_name)\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=['Competitor', 'Tweet'])\n",
    "    print(f\"  Dropped {before - len(df_clean)} rows with invalid competitor/tweet\")\n",
    "    \n",
    "    # Clean tweet text\n",
    "    df_clean['Tweet'] = df_clean['Tweet'].astype(str).str.strip()\n",
    "    df_clean = df_clean[df_clean['Tweet'].str.len() > 0]\n",
    "    \n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  Final rows: {len(df_clean)}\")\n",
    "    \n",
    "    # Validation: Check competitor distribution\n",
    "    comp_dist = df_clean['Competitor'].value_counts()\n",
    "    print(f\"  Unique competitors: {len(comp_dist)}\")\n",
    "    print(f\"  Most common: {comp_dist.index[0]} ({comp_dist.iloc[0]} samples)\")\n",
    "    \n",
    "    if 'SENTIMENT' in df_clean.columns:\n",
    "        sent_dist = df_clean['SENTIMENT'].value_counts().sort_index()\n",
    "        print(f\"  Sentiment distribution: {dict(sent_dist)}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Prepare all datasets\n",
    "df_large_clean = prepare_dataset(df_large, \"Large dataset\")\n",
    "df_test_clean = prepare_dataset(df_test, \"Test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhanced Multi-Competitor Extraction (Regex-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_competitors(tweet_text):\n",
    "    \"\"\"\n",
    "    Extract ALL competitors mentioned in tweet using regex.\n",
    "    Returns list of competitor names.\n",
    "    \"\"\"\n",
    "    found_competitors = set()\n",
    "    tweet_lower = tweet_text.lower()\n",
    "    \n",
    "    # Define patterns for each competitor\n",
    "    patterns = {\n",
    "        'Burger King': [r'\\bburger\\s*king\\b', r'\\bbk\\b'],\n",
    "        'Deliveroo': [r'\\bdeliveroo\\b'],\n",
    "        \"Domino's\": [r'\\bdomino(?:s|\\'s)?\\b'],\n",
    "        'Five Guys': [r'\\bfive\\s*guys\\b'],\n",
    "        'Greggs': [r'\\bgreggs?\\b'],\n",
    "        'Just Eat': [r'\\bjust\\s*eat\\b'],\n",
    "        'KFC': [r'\\bkfc\\b', r'\\bkentucky\\s*fried\\s*chicken\\b', r'@kfc'],\n",
    "        \"McDonald's\": [r'\\bmcdonald(?:s|\\'s)?\\b', r'\\bmaccies\\b', r'\\bmaccas\\b', r'\\bmcdonalds\\b', r'@mcdonald'],\n",
    "        \"Nando's\": [r'\\bnando(?:s|\\'s)?\\b', r'@nando'],\n",
    "        \"Papa John's\": [r'\\bpapa\\s*john(?:s|\\'s)?\\b', r'@papajohn'],\n",
    "        'Pizza Hut': [r'\\bpizza\\s*hut\\b', r'@pizzahut'],\n",
    "        'Pret a Manger': [r'\\bpret(?:\\s*a\\s*manger)?\\b', r'@pret'],\n",
    "        'Taco Bell': [r'\\btaco\\s*bell\\b', r'@tacobell'],\n",
    "        'Uber Eats': [r'\\buber\\s*eats\\b', r'@ubereats']\n",
    "    }\n",
    "    \n",
    "    for competitor, pattern_list in patterns.items():\n",
    "        for pattern in pattern_list:\n",
    "            if re.search(pattern, tweet_lower):\n",
    "                found_competitors.add(competitor)\n",
    "                break\n",
    "    \n",
    "    return list(found_competitors)\n",
    "\n",
    "# Test extraction\n",
    "test_tweets = [\n",
    "    \"I love KFC's chicken!\",\n",
    "    \"McDonald's and Burger King are both great\",\n",
    "    \"Just ordered from @Deliveroo\",\n",
    "    \"Pret a Manger, Nando's, and KFC all have good food\"\n",
    "]\n",
    "\n",
    "print(\"Testing competitor extraction:\\n\")\n",
    "for tweet in test_tweets:\n",
    "    comps = extract_all_competitors(tweet)\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Found: {comps}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified split\n",
    "train_df, val_df = train_test_split(\n",
    "    df_large_clean,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=df_large_clean['Competitor']\n",
    ")\n",
    "\n",
    "print(f\"Dataset Split:\")\n",
    "print(f\"  Training: {len(train_df)} samples\")\n",
    "print(f\"  Validation: {len(val_df)} samples\")\n",
    "print(f\"  Test: {len(df_test_clean)} samples\")\n",
    "\n",
    "print(f\"\\nTraining set competitor distribution:\")\n",
    "print(train_df['Competitor'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. NER Model - Single-Label Classification (FIXED)\n",
    "\n",
    "**Key Change**: Instead of multi-label, we do **14-class classification** to predict which competitor the tweet is primarily about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitorClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for single-label competitor classification.\n",
    "    Task: Given tweet, predict which competitor it's about (0-13)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Create label mapping\n",
    "        self.competitor_to_idx = {comp: idx for idx, comp in enumerate(COMPETITORS)}\n",
    "        \n",
    "        # Validate: check if all competitors in data are in our list\n",
    "        unique_comps = self.data['Competitor'].unique()\n",
    "        print(f\"\\nDataset has {len(unique_comps)} unique competitors:\")\n",
    "        for comp in unique_comps:\n",
    "            if comp in self.competitor_to_idx:\n",
    "                count = (self.data['Competitor'] == comp).sum()\n",
    "                print(f\"  ✓ {comp}: {count} samples (label {self.competitor_to_idx[comp]})\")\n",
    "            else:\n",
    "                print(f\"  ✗ {comp}: NOT IN COMPETITOR LIST!\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        tweet = row['Tweet']\n",
    "        competitor = row['Competitor']\n",
    "        \n",
    "        # Get label index\n",
    "        label = self.competitor_to_idx[competitor]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"✓ CompetitorClassificationDataset defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and create datasets\n",
    "print(f\"Loading NER tokenizer: {NER_MODEL_NAME}\")\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_NAME)\n",
    "\n",
    "print(\"\\nCreating NER datasets...\")\n",
    "ner_train_dataset = CompetitorClassificationDataset(train_df, ner_tokenizer, MAX_SEQ_LENGTH)\n",
    "ner_val_dataset = CompetitorClassificationDataset(val_df, ner_tokenizer, MAX_SEQ_LENGTH)\n",
    "ner_test_dataset = CompetitorClassificationDataset(df_test_clean, ner_tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "ner_train_loader = DataLoader(ner_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "ner_val_loader = DataLoader(ner_val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\n",
    "ner_test_loader = DataLoader(ner_test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\n✓ DataLoaders created (batch size: {BATCH_SIZE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for NER (to handle imbalance)\n",
    "train_labels = [ner_train_dataset.competitor_to_idx[comp] for comp in train_df['Competitor']]\n",
    "ner_class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.arange(len(COMPETITORS)),\n",
    "    y=train_labels\n",
    ")\n",
    "ner_class_weights = torch.tensor(ner_class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(\"NER Class Weights:\")\n",
    "for i, comp in enumerate(COMPETITORS):\n",
    "    print(f\"  {comp:20s}: {ner_class_weights[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner_classifier(model, train_loader, val_loader, epochs, learning_rate, class_weights):\n",
    "    \"\"\"\n",
    "    Train NER model with proper validation\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs // GRAD_ACCUM_STEPS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_f1': []}\n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    print(f\"\\nTraining NER model...\")\n",
    "    print(f\"  Epochs: {epochs}, Steps: {total_steps}, Warmup: {warmup_steps}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for step, batch in enumerate(train_pbar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs.logits, labels) / GRAD_ACCUM_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item() * GRAD_ACCUM_STEPS:.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            print(f\"  ✓ New best F1! Saving model...\")\n",
    "            torch.save(model.state_dict(), f'{MODEL_SAVE_DIR}/ner_best_model.pt')\n",
    "        print()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\n✓ Training complete! Best val F1: {best_val_f1:.4f}\")\n",
    "    return model, history\n",
    "\n",
    "print(\"✓ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train NER model\n",
    "print(f\"Initializing NER model: {NER_MODEL_NAME}\")\n",
    "ner_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    NER_MODEL_NAME,\n",
    "    num_labels=len(COMPETITORS)\n",
    ")\n",
    "\n",
    "# Train\n",
    "ner_model, ner_history = train_ner_classifier(\n",
    "    ner_model,\n",
    "    ner_train_loader,\n",
    "    ner_val_loader,\n",
    "    epochs=NER_EPOCHS,\n",
    "    learning_rate=NER_LEARNING_RATE,\n",
    "    class_weights=ner_class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot NER training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(ner_history['train_loss'], label='Train', marker='o')\n",
    "axes[0].plot(ner_history['val_loss'], label='Val', marker='s')\n",
    "axes[0].set_title('NER - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(ner_history['val_accuracy'], marker='o', color='blue')\n",
    "axes[1].set_title('NER - Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(ner_history['val_f1'], marker='o', color='green')\n",
    "axes[2].set_title('NER - Validation F1', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/ner_training.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Best validation F1: {max(ner_history['val_f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sentiment Model (Same as Before)\n",
    "\n",
    "This part works fine - using competitor-aware sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SENTIMENT MODEL CODE - SAME AS ORIGINAL NOTEBOOK]\n",
    "# Copy cells 75-85 from original notebook here\n",
    "# (SentimentDataset, training function, etc.)\n",
    "\n",
    "print(\"✓ Sentiment model section ready\")\n",
    "print(\"  Copy sentiment model cells from original notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integrated Pipeline - FIXED\n",
    "\n",
    "Combines:\n",
    "1. NER classifier (primary competitor)\n",
    "2. Regex extraction (all mentioned competitors)\n",
    "3. Sentiment model (per competitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipeline(tweet_text, ner_model, sentiment_model, ner_tokenizer, sentiment_tokenizer):\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    1. Use NER model to predict primary competitor\n",
    "    2. Use regex to extract ALL mentioned competitors\n",
    "    3. Combine both (union)\n",
    "    4. For each competitor, predict sentiment\n",
    "    \"\"\"\n",
    "    ner_model.eval()\n",
    "    sentiment_model.eval()\n",
    "    \n",
    "    # Step 1: NER model prediction\n",
    "    encoding = ner_tokenizer(\n",
    "        tweet_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            outputs = ner_model(encoding['input_ids'].to(device), encoding['attention_mask'].to(device))\n",
    "    \n",
    "    predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
    "    primary_competitor = COMPETITORS[predicted_idx]\n",
    "    \n",
    "    # Step 2: Regex extraction\n",
    "    regex_competitors = extract_all_competitors(tweet_text)\n",
    "    \n",
    "    # Step 3: Combine (union)\n",
    "    all_competitors = set([primary_competitor] + regex_competitors)\n",
    "    \n",
    "    # Step 4: Sentiment for each\n",
    "    results = []\n",
    "    for competitor in all_competitors:\n",
    "        text = f\"{tweet_text} This tweet is about {competitor}.\"\n",
    "        \n",
    "        sentiment_encoding = sentiment_tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                sentiment_outputs = sentiment_model(\n",
    "                    sentiment_encoding['input_ids'].to(device),\n",
    "                    sentiment_encoding['attention_mask'].to(device)\n",
    "                )\n",
    "        \n",
    "        predicted_sentiment = torch.argmax(sentiment_outputs.logits, dim=1).item()\n",
    "        results.append((competitor, predicted_sentiment))\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Integrated pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "**Key Fixes:**\n",
    "1. ✅ NER changed from multi-label to single-label (14-class classification)\n",
    "2. ✅ Added data validation at every step\n",
    "3. ✅ Enhanced regex extraction for multi-competitor tweets\n",
    "4. ✅ Class weights for imbalanced data\n",
    "5. ✅ Proper label creation with debugging\n",
    "\n",
    "**Expected Performance:**\n",
    "- NER F1 should now be >0.70 (not 0.0000!)\n",
    "- Sentiment F1 should be >0.70\n",
    "- Combined pipeline should handle multi-competitor tweets correctly"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
