{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Competitor NER & Sentiment Analysis - COMPLETE FIXED VERSION\n",
    "\n",
    "**Fixed Issues:**\n",
    "1. ‚úÖ AdamW import (now uses `torch.optim.AdamW` instead of transformers)\n",
    "2. ‚úÖ CSV encoding (auto-detects and converts to UTF-8)\n",
    "3. ‚úÖ Excel output (full formatted .xlsx export)\n",
    "4. ‚úÖ NER model (single-label classification that actually works)\n",
    "5. ‚úÖ Complete sentiment model implementation\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load and convert CSVs to UTF-8\n",
    "2. Train NER classifier (14-class)\n",
    "3. Train sentiment model (3-class)\n",
    "4. Generate predictions\n",
    "5. Export to formatted Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q transformers datasets torch torchvision accelerate\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install -q sentencepiece protobuf\n",
    "!pip install -q openpyxl xlsxwriter  # For Excel export\n",
    "!pip install -q chardet  # For encoding detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "import warnings\n",
    "import chardet\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW  # FIXED: Use torch.optim.AdamW instead of transformers\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, f1_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì Using AdamW from: torch.optim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and configure device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    gpu_memory_gb = gpu_props.total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_props.name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
    "    \n",
    "    # Adaptive batch size\n",
    "    if gpu_memory_gb >= 15:\n",
    "        BATCH_SIZE = 16\n",
    "        GRAD_ACCUM_STEPS = 2\n",
    "    else:\n",
    "        BATCH_SIZE = 8\n",
    "        GRAD_ACCUM_STEPS = 4\n",
    "else:\n",
    "    BATCH_SIZE = 4\n",
    "    GRAD_ACCUM_STEPS = 8\n",
    "\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "print(f\"\\nBatch size: {BATCH_SIZE}, Accumulation: {GRAD_ACCUM_STEPS}, Effective: {EFFECTIVE_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p '/content/drive/MyDrive/KFC_ML_Models'\n",
    "!mkdir -p '/content/results'\n",
    "\n",
    "MODEL_SAVE_DIR = '/content/drive/MyDrive/KFC_ML_Models'\n",
    "RESULTS_DIR = '/content/results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitor list (14 total)\n",
    "COMPETITORS = [\n",
    "    'Burger King', 'Deliveroo', \"Domino's\", 'Five Guys', 'Greggs',\n",
    "    'Just Eat', 'KFC', \"McDonald's\", \"Nando's\", \"Papa John's\",\n",
    "    'Pizza Hut', 'Pret a Manger', 'Taco Bell', 'Uber Eats'\n",
    "]\n",
    "\n",
    "SENTIMENT_MAP = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "# Model configs\n",
    "NER_MODEL_NAME = 'bert-base-uncased'\n",
    "SENTIMENT_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_SEQ_LENGTH = 128\n",
    "NER_LEARNING_RATE = 2e-5\n",
    "SENTIMENT_LEARNING_RATE = 2e-5\n",
    "NER_EPOCHS = 5\n",
    "SENTIMENT_EPOCHS = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "print(f\"‚úì Configuration loaded\")\n",
    "print(f\"  Competitors: {len(COMPETITORS)}\")\n",
    "print(f\"  NER Model: {NER_MODEL_NAME}\")\n",
    "print(f\"  Sentiment Model: {SENTIMENT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CSV Loading with UTF-8 Conversion (FIXED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    \"\"\"\n",
    "    Detect file encoding using chardet.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(100000))  # Read first 100KB\n",
    "    return result['encoding'], result['confidence']\n",
    "\n",
    "def convert_to_utf8(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Convert CSV file to UTF-8 encoding.\n",
    "    If output_file is None, overwrites input_file.\n",
    "    \"\"\"\n",
    "    if output_file is None:\n",
    "        output_file = input_file\n",
    "    \n",
    "    # Detect encoding\n",
    "    encoding, confidence = detect_encoding(input_file)\n",
    "    print(f\"  Detected encoding: {encoding} (confidence: {confidence:.2%})\")\n",
    "    \n",
    "    if encoding and encoding.lower() != 'utf-8':\n",
    "        # Read with detected encoding\n",
    "        try:\n",
    "            with open(input_file, 'r', encoding=encoding, errors='replace') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Write as UTF-8\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            print(f\"  ‚úì Converted to UTF-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Conversion failed: {e}\")\n",
    "            print(f\"  Trying with 'utf-8' and error handling...\")\n",
    "    else:\n",
    "        print(f\"  ‚úì Already UTF-8\")\n",
    "\n",
    "def load_csv_safe(file_path):\n",
    "    \"\"\"\n",
    "    Load CSV with automatic UTF-8 conversion.\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading: {file_path}\")\n",
    "    \n",
    "    # Convert to UTF-8 first\n",
    "    convert_to_utf8(file_path)\n",
    "    \n",
    "    # Load with pandas\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)\n",
    "        print(f\"  ‚úì Loaded {len(df)} rows\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Error loading with UTF-8: {e}\")\n",
    "        print(f\"  Trying with encoding detection...\")\n",
    "        encoding, _ = detect_encoding(file_path)\n",
    "        df = pd.read_csv(file_path, encoding=encoding, low_memory=False, errors='replace')\n",
    "        print(f\"  ‚úì Loaded {len(df)} rows with {encoding}\")\n",
    "        return df\n",
    "\n",
    "print(\"‚úì CSV loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload CSV files\n",
    "from google.colab import files\n",
    "print(\"Please upload your CSV files:\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets with UTF-8 conversion\n",
    "print(\"Loading and converting datasets to UTF-8...\")\n",
    "\n",
    "df_large = load_csv_safe('KFC_social_data.xlsx - Sheet1.csv')\n",
    "df_train_sample = load_csv_safe('KFC_training_sample.csv')\n",
    "df_test = load_csv_safe('KFC_test_sample.csv')\n",
    "df_test_pred = load_csv_safe('KFC_test_sample_for_prediction.csv')\n",
    "\n",
    "print(f\"\\n‚úì All datasets loaded and converted to UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentiment(value):\n",
    "    \"\"\"Extract numeric sentiment (0, 1, 2)\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    \n",
    "    if isinstance(value, (int, float)):\n",
    "        if value in [0, 1, 2]:\n",
    "            return int(value)\n",
    "        return None\n",
    "    \n",
    "    value_str = str(value).strip().lower()\n",
    "    if value_str in ['0', 'negative']: return 0\n",
    "    elif value_str in ['1', 'neutral']: return 1\n",
    "    elif value_str in ['2', 'positive']: return 2\n",
    "    \n",
    "    match = re.match(r'^(\\d)', value_str)\n",
    "    if match:\n",
    "        digit = int(match.group(1))\n",
    "        if digit in [0, 1, 2]:\n",
    "            return digit\n",
    "    \n",
    "    return None\n",
    "\n",
    "def normalize_competitor_name(comp_str):\n",
    "    \"\"\"Normalize competitor names\"\"\"\n",
    "    if pd.isna(comp_str) or not comp_str:\n",
    "        return None\n",
    "    \n",
    "    comp_str = str(comp_str).strip()\n",
    "    \n",
    "    if comp_str in COMPETITORS:\n",
    "        return comp_str\n",
    "    \n",
    "    for comp in COMPETITORS:\n",
    "        if comp.lower() == comp_str.lower():\n",
    "            return comp\n",
    "    \n",
    "    return None\n",
    "\n",
    "def prepare_dataset(df, name=\"dataset\"):\n",
    "    \"\"\"Clean and prepare dataset\"\"\"\n",
    "    print(f\"\\nPreparing {name}...\")\n",
    "    print(f\"  Initial rows: {len(df)}\")\n",
    "    \n",
    "    # Select columns\n",
    "    essential_cols = ['Competitor', 'Tweet', 'SENTIMENT']\n",
    "    if 'Tweet' not in df.columns:\n",
    "        if 'Full Text' in df.columns:\n",
    "            df['Tweet'] = df['Full Text']\n",
    "        elif 'Snippet' in df.columns:\n",
    "            df['Tweet'] = df['Snippet']\n",
    "    \n",
    "    available_cols = [col for col in essential_cols if col in df.columns]\n",
    "    metadata_cols = [col for col in ['Impact', 'Impressions', 'Reach (new)', 'Date', 'Url'] if col in df.columns]\n",
    "    \n",
    "    df_clean = df[available_cols + metadata_cols].copy()\n",
    "    \n",
    "    # Clean sentiment\n",
    "    if 'SENTIMENT' in df_clean.columns:\n",
    "        df_clean['SENTIMENT'] = df_clean['SENTIMENT'].apply(clean_sentiment)\n",
    "        before = len(df_clean)\n",
    "        df_clean = df_clean.dropna(subset=['SENTIMENT'])\n",
    "        print(f\"  Dropped {before - len(df_clean)} rows with invalid sentiment\")\n",
    "        df_clean['SENTIMENT'] = df_clean['SENTIMENT'].astype(int)\n",
    "    \n",
    "    # Clean competitor names\n",
    "    df_clean['Competitor'] = df_clean['Competitor'].apply(normalize_competitor_name)\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=['Competitor', 'Tweet'])\n",
    "    print(f\"  Dropped {before - len(df_clean)} rows with invalid competitor/tweet\")\n",
    "    \n",
    "    # Clean tweet text\n",
    "    df_clean['Tweet'] = df_clean['Tweet'].astype(str).str.strip()\n",
    "    df_clean = df_clean[df_clean['Tweet'].str.len() > 0]\n",
    "    \n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  Final rows: {len(df_clean)}\")\n",
    "    print(f\"  Unique competitors: {df_clean['Competitor'].nunique()}\")\n",
    "    \n",
    "    if 'SENTIMENT' in df_clean.columns:\n",
    "        sent_dist = df_clean['SENTIMENT'].value_counts().sort_index()\n",
    "        print(f\"  Sentiment distribution: {dict(sent_dist)}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Prepare datasets\n",
    "df_large_clean = prepare_dataset(df_large, \"Large dataset\")\n",
    "df_test_clean = prepare_dataset(df_test, \"Test dataset\")\n",
    "\n",
    "# For prediction data (no sentiment)\n",
    "if 'SENTIMENT' not in df_test_pred.columns:\n",
    "    df_test_pred['SENTIMENT'] = 1  # Dummy value\n",
    "df_test_pred_clean = prepare_dataset(df_test_pred, \"Test prediction dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Competitor Extraction (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_competitors(tweet_text):\n",
    "    \"\"\"\n",
    "    Extract ALL competitors mentioned using regex.\n",
    "    \"\"\"\n",
    "    found_competitors = set()\n",
    "    tweet_lower = tweet_text.lower()\n",
    "    \n",
    "    patterns = {\n",
    "        'Burger King': [r'\\bburger\\s*king\\b', r'\\bbk\\b'],\n",
    "        'Deliveroo': [r'\\bdeliveroo\\b'],\n",
    "        \"Domino's\": [r'\\bdomino(?:s|\\'s)?\\b'],\n",
    "        'Five Guys': [r'\\bfive\\s*guys\\b'],\n",
    "        'Greggs': [r'\\bgreggs?\\b'],\n",
    "        'Just Eat': [r'\\bjust\\s*eat\\b'],\n",
    "        'KFC': [r'\\bkfc\\b', r'\\bkentucky\\s*fried\\s*chicken\\b', r'@kfc'],\n",
    "        \"McDonald's\": [r'\\bmcdonald(?:s|\\'s)?\\b', r'\\bmaccies\\b', r'\\bmaccas\\b', r'\\bmcdonalds\\b', r'@mcdonald'],\n",
    "        \"Nando's\": [r'\\bnando(?:s|\\'s)\\b', r'@nando'],\n",
    "        \"Papa John's\": [r'\\bpapa\\s*john(?:s|\\'s)?\\b', r'@papajohn'],\n",
    "        'Pizza Hut': [r'\\bpizza\\s*hut\\b', r'@pizzahut'],\n",
    "        'Pret a Manger': [r'\\bpret(?:\\s*a\\s*manger)?\\b', r'@pret'],\n",
    "        'Taco Bell': [r'\\btaco\\s*bell\\b', r'@tacobell'],\n",
    "        'Uber Eats': [r'\\buber\\s*eats\\b', r'@ubereats']\n",
    "    }\n",
    "    \n",
    "    for competitor, pattern_list in patterns.items():\n",
    "        for pattern in pattern_list:\n",
    "            if re.search(pattern, tweet_lower):\n",
    "                found_competitors.add(competitor)\n",
    "                break\n",
    "    \n",
    "    return list(found_competitors)\n",
    "\n",
    "# Test\n",
    "test_tweet = \"I love KFC's chicken but McDonald's has better fries!\"\n",
    "print(f\"Test tweet: {test_tweet}\")\n",
    "print(f\"Found competitors: {extract_all_competitors(test_tweet)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split\n",
    "train_df, val_df = train_test_split(\n",
    "    df_large_clean,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=df_large_clean['Competitor']\n",
    ")\n",
    "\n",
    "print(f\"Dataset Split:\")\n",
    "print(f\"  Training: {len(train_df)} samples\")\n",
    "print(f\"  Validation: {len(val_df)} samples\")\n",
    "print(f\"  Test: {len(df_test_clean)} samples\")\n",
    "print(f\"\\nTop 5 competitors in training set:\")\n",
    "print(train_df['Competitor'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. NER Model - Single-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitorDataset(Dataset):\n",
    "    \"\"\"Dataset for competitor classification (0-13)\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.competitor_to_idx = {comp: idx for idx, comp in enumerate(COMPETITORS)}\n",
    "        \n",
    "        # Validation\n",
    "        unique_comps = self.data['Competitor'].unique()\n",
    "        print(f\"\\nDataset has {len(unique_comps)} unique competitors:\")\n",
    "        for comp in unique_comps:\n",
    "            if comp in self.competitor_to_idx:\n",
    "                count = (self.data['Competitor'] == comp).sum()\n",
    "                print(f\"  ‚úì {comp}: {count} samples (label {self.competitor_to_idx[comp]})\")\n",
    "            else:\n",
    "                print(f\"  ‚úó {comp}: NOT IN COMPETITOR LIST!\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        tweet = row['Tweet']\n",
    "        competitor = row['Competitor']\n",
    "        label = self.competitor_to_idx[competitor]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úì CompetitorDataset defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NER tokenizer\n",
    "print(f\"Loading NER tokenizer: {NER_MODEL_NAME}\")\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating NER datasets...\")\n",
    "ner_train_dataset = CompetitorDataset(train_df, ner_tokenizer, MAX_SEQ_LENGTH)\n",
    "ner_val_dataset = CompetitorDataset(val_df, ner_tokenizer, MAX_SEQ_LENGTH)\n",
    "ner_test_dataset = CompetitorDataset(df_test_clean, ner_tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "# DataLoaders\n",
    "ner_train_loader = DataLoader(ner_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "ner_val_loader = DataLoader(ner_val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\n",
    "ner_test_loader = DataLoader(ner_test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\n‚úì DataLoaders created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights for imbalanced data\n",
    "train_labels = [ner_train_dataset.competitor_to_idx[comp] for comp in train_df['Competitor']]\n",
    "ner_class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.arange(len(COMPETITORS)),\n",
    "    y=train_labels\n",
    ")\n",
    "ner_class_weights = torch.tensor(ner_class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(\"NER Class Weights:\")\n",
    "for i, comp in enumerate(COMPETITORS):\n",
    "    print(f\"  {comp:20s}: {ner_class_weights[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, class_weights, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Training function with torch.optim.AdamW (FIXED)\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # FIXED: Use torch.optim.AdamW\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs // GRAD_ACCUM_STEPS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_f1': []}\n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    print(f\"  Epochs: {epochs}, Steps: {total_steps}, Warmup: {warmup_steps}\")\n",
    "    print(f\"  Using optimizer: torch.optim.AdamW\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for step, batch in enumerate(train_pbar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs.logits, labels) / GRAD_ACCUM_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item() * GRAD_ACCUM_STEPS:.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            print(f\"  ‚úì New best F1! Saving model...\")\n",
    "            torch.save(model.state_dict(), f'{MODEL_SAVE_DIR}/{model_name}_best.pt')\n",
    "        print()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\n‚úì Training complete! Best val F1: {best_val_f1:.4f}\")\n",
    "    return model, history\n",
    "\n",
    "print(\"‚úì Training function defined (using torch.optim.AdamW)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NER model\n",
    "print(f\"Initializing NER model: {NER_MODEL_NAME}\")\n",
    "ner_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    NER_MODEL_NAME,\n",
    "    num_labels=len(COMPETITORS)\n",
    ")\n",
    "\n",
    "ner_model, ner_history = train_model(\n",
    "    ner_model,\n",
    "    ner_train_loader,\n",
    "    ner_val_loader,\n",
    "    epochs=NER_EPOCHS,\n",
    "    learning_rate=NER_LEARNING_RATE,\n",
    "    class_weights=ner_class_weights,\n",
    "    model_name=\"ner_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot NER history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(ner_history['train_loss'], label='Train', marker='o')\n",
    "axes[0].plot(ner_history['val_loss'], label='Val', marker='s')\n",
    "axes[0].set_title('NER - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(ner_history['val_accuracy'], marker='o', color='blue')\n",
    "axes[1].set_title('NER - Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(ner_history['val_f1'], marker='o', color='green')\n",
    "axes[2].set_title('NER - Validation F1', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/ner_training.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Best NER F1: {max(ner_history['val_f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Competitor-aware sentiment dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        tweet = row['Tweet']\n",
    "        competitor = row['Competitor']\n",
    "        sentiment = row['SENTIMENT']\n",
    "        \n",
    "        # Contextualized input\n",
    "        text = f\"{tweet} This tweet is about {competitor}.\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(sentiment, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úì SentimentDataset defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentiment tokenizer\n",
    "print(f\"Loading Sentiment tokenizer: {SENTIMENT_MODEL_NAME}\")\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(SENTIMENT_MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating Sentiment datasets...\")\n",
    "sentiment_train_dataset = SentimentDataset(train_df, sentiment_tokenizer, MAX_SEQ_LENGTH)\n",
    "sentiment_val_dataset = SentimentDataset(val_df, sentiment_tokenizer, MAX_SEQ_LENGTH)\n",
    "sentiment_test_dataset = SentimentDataset(df_test_clean, sentiment_tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "print(f\"  Train: {len(sentiment_train_dataset)} samples\")\n",
    "print(f\"  Val: {len(sentiment_val_dataset)} samples\")\n",
    "print(f\"  Test: {len(sentiment_test_dataset)} samples\")\n",
    "\n",
    "# DataLoaders\n",
    "sentiment_train_loader = DataLoader(sentiment_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "sentiment_val_loader = DataLoader(sentiment_val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\n",
    "sentiment_test_loader = DataLoader(sentiment_test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\n‚úì DataLoaders created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment class weights\n",
    "sentiment_class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.arange(3),\n",
    "    y=train_df['SENTIMENT'].values\n",
    ")\n",
    "sentiment_class_weights = torch.tensor(sentiment_class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(\"Sentiment Class Weights:\")\n",
    "for i, label in SENTIMENT_MAP.items():\n",
    "    print(f\"  {label:8s}: {sentiment_class_weights[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Sentiment model\n",
    "print(f\"Initializing Sentiment model: {SENTIMENT_MODEL_NAME}\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    SENTIMENT_MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "sentiment_model, sentiment_history = train_model(\n",
    "    sentiment_model,\n",
    "    sentiment_train_loader,\n",
    "    sentiment_val_loader,\n",
    "    epochs=SENTIMENT_EPOCHS,\n",
    "    learning_rate=SENTIMENT_LEARNING_RATE,\n",
    "    class_weights=sentiment_class_weights,\n",
    "    model_name=\"sentiment_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sentiment history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(sentiment_history['train_loss'], label='Train', marker='o')\n",
    "axes[0].plot(sentiment_history['val_loss'], label='Val', marker='s')\n",
    "axes[0].set_title('Sentiment - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(sentiment_history['val_accuracy'], marker='o', color='blue')\n",
    "axes[1].set_title('Sentiment - Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(sentiment_history['val_f1'], marker='o', color='green')\n",
    "axes[2].set_title('Sentiment - Validation F1', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/sentiment_training.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Best Sentiment F1: {max(sentiment_history['val_f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integrated Pipeline & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet_text, ner_model, sentiment_model, ner_tokenizer, sentiment_tokenizer):\n",
    "    \"\"\"\n",
    "    Full pipeline: NER + Regex ‚Üí Sentiment per competitor\n",
    "    \"\"\"\n",
    "    ner_model.eval()\n",
    "    sentiment_model.eval()\n",
    "    \n",
    "    # NER prediction\n",
    "    encoding = ner_tokenizer(\n",
    "        tweet_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            outputs = ner_model(encoding['input_ids'].to(device), encoding['attention_mask'].to(device))\n",
    "    \n",
    "    predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
    "    primary_competitor = COMPETITORS[predicted_idx]\n",
    "    \n",
    "    # Regex extraction\n",
    "    regex_competitors = extract_all_competitors(tweet_text)\n",
    "    \n",
    "    # Combine\n",
    "    all_competitors = set([primary_competitor] + regex_competitors)\n",
    "    \n",
    "    # Sentiment for each\n",
    "    results = []\n",
    "    for competitor in all_competitors:\n",
    "        text = f\"{tweet_text} This tweet is about {competitor}.\"\n",
    "        \n",
    "        sentiment_encoding = sentiment_tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                sentiment_outputs = sentiment_model(\n",
    "                    sentiment_encoding['input_ids'].to(device),\n",
    "                    sentiment_encoding['attention_mask'].to(device)\n",
    "                )\n",
    "        \n",
    "        predicted_sentiment = torch.argmax(sentiment_outputs.logits, dim=1).item()\n",
    "        results.append((competitor, predicted_sentiment))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "test_tweet = \"KFC's chicken is amazing but McDonald's has terrible service\"\n",
    "print(f\"Test tweet: {test_tweet}\")\n",
    "predictions = predict_tweet(test_tweet, ner_model, sentiment_model, ner_tokenizer, sentiment_tokenizer)\n",
    "for comp, sent in predictions:\n",
    "    print(f\"  {comp}: {SENTIMENT_MAP[sent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(df, ner_model, sentiment_model, ner_tokenizer, sentiment_tokenizer):\n",
    "    \"\"\"\n",
    "    Process entire dataset with pipeline.\n",
    "    Returns DataFrame with one row per detected competitor.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing tweets\"):\n",
    "        tweet = row['Tweet']\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = predict_tweet(tweet, ner_model, sentiment_model, ner_tokenizer, sentiment_tokenizer)\n",
    "        \n",
    "        # Create row for each detected competitor\n",
    "        for competitor, sentiment in predictions:\n",
    "            result_row = {\n",
    "                'Competitor': competitor,\n",
    "                'Tweet': tweet,\n",
    "                'Predicted_Sentiment': sentiment,\n",
    "                'Sentiment_Label': SENTIMENT_MAP[sentiment]\n",
    "            }\n",
    "            \n",
    "            # Add metadata\n",
    "            for col in ['Impact', 'Impressions', 'Reach (new)', 'Date', 'Url']:\n",
    "                if col in row.index and pd.notna(row[col]):\n",
    "                    result_row[col] = row[col]\n",
    "            \n",
    "            results.append(result_row)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"‚úì Batch processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test data\n",
    "print(\"Generating predictions for test data...\\n\")\n",
    "\n",
    "# Load best models\n",
    "ner_model.load_state_dict(torch.load(f'{MODEL_SAVE_DIR}/ner_model_best.pt'))\n",
    "sentiment_model.load_state_dict(torch.load(f'{MODEL_SAVE_DIR}/sentiment_model_best.pt'))\n",
    "\n",
    "# Process\n",
    "predictions_df = process_dataset(\n",
    "    df_test_pred_clean,\n",
    "    ner_model,\n",
    "    sentiment_model,\n",
    "    ner_tokenizer,\n",
    "    sentiment_tokenizer\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Generated {len(predictions_df)} predictions from {len(df_test_pred_clean)} tweets\")\n",
    "print(f\"  Average competitors per tweet: {len(predictions_df) / len(df_test_pred_clean):.2f}\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Excel Export with Formatting (FIXED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_excel(df, filename, include_summary=True):\n",
    "    \"\"\"\n",
    "    Export predictions to formatted Excel file.\n",
    "    \"\"\"\n",
    "    filepath = f'{RESULTS_DIR}/{filename}'\n",
    "    \n",
    "    # Create Excel writer\n",
    "    with pd.ExcelWriter(filepath, engine='xlsxwriter') as writer:\n",
    "        # Write main data\n",
    "        df.to_excel(writer, sheet_name='Predictions', index=False)\n",
    "        \n",
    "        # Get workbook and worksheet\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['Predictions']\n",
    "        \n",
    "        # Define formats\n",
    "        header_format = workbook.add_format({\n",
    "            'bold': True,\n",
    "            'text_wrap': True,\n",
    "            'valign': 'top',\n",
    "            'fg_color': '#D7E4BD',\n",
    "            'border': 1\n",
    "        })\n",
    "        \n",
    "        # Sentiment color formats\n",
    "        negative_format = workbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})\n",
    "        neutral_format = workbook.add_format({'bg_color': '#FFEB9C', 'font_color': '#9C6500'})\n",
    "        positive_format = workbook.add_format({'bg_color': '#C6EFCE', 'font_color': '#006100'})\n",
    "        \n",
    "        # Apply header format\n",
    "        for col_num, value in enumerate(df.columns.values):\n",
    "            worksheet.write(0, col_num, value, header_format)\n",
    "        \n",
    "        # Set column widths\n",
    "        worksheet.set_column('A:A', 15)  # Competitor\n",
    "        worksheet.set_column('B:B', 60)  # Tweet\n",
    "        worksheet.set_column('C:C', 12)  # Predicted_Sentiment\n",
    "        worksheet.set_column('D:D', 15)  # Sentiment_Label\n",
    "        worksheet.set_column('E:Z', 12)  # Other columns\n",
    "        \n",
    "        # Apply conditional formatting for sentiment\n",
    "        if 'Sentiment_Label' in df.columns:\n",
    "            sentiment_col = df.columns.get_loc('Sentiment_Label')\n",
    "            \n",
    "            # Negative\n",
    "            worksheet.conditional_format(1, sentiment_col, len(df), sentiment_col, {\n",
    "                'type': 'text',\n",
    "                'criteria': 'containing',\n",
    "                'value': 'negative',\n",
    "                'format': negative_format\n",
    "            })\n",
    "            \n",
    "            # Neutral\n",
    "            worksheet.conditional_format(1, sentiment_col, len(df), sentiment_col, {\n",
    "                'type': 'text',\n",
    "                'criteria': 'containing',\n",
    "                'value': 'neutral',\n",
    "                'format': neutral_format\n",
    "            })\n",
    "            \n",
    "            # Positive\n",
    "            worksheet.conditional_format(1, sentiment_col, len(df), sentiment_col, {\n",
    "                'type': 'text',\n",
    "                'criteria': 'containing',\n",
    "                'value': 'positive',\n",
    "                'format': positive_format\n",
    "            })\n",
    "        \n",
    "        # Add summary sheet if requested\n",
    "        if include_summary:\n",
    "            summary_data = []\n",
    "            \n",
    "            # Overall stats\n",
    "            summary_data.append(['Metric', 'Value'])\n",
    "            summary_data.append(['Total Predictions', len(df)])\n",
    "            summary_data.append(['Unique Tweets', df['Tweet'].nunique()])\n",
    "            summary_data.append(['Unique Competitors', df['Competitor'].nunique()])\n",
    "            summary_data.append([''])\n",
    "            \n",
    "            # Sentiment distribution\n",
    "            summary_data.append(['Sentiment', 'Count', 'Percentage'])\n",
    "            sent_dist = df['Sentiment_Label'].value_counts()\n",
    "            for sent, count in sent_dist.items():\n",
    "                pct = count / len(df) * 100\n",
    "                summary_data.append([sent, count, f'{pct:.1f}%'])\n",
    "            \n",
    "            summary_data.append([''])\n",
    "            \n",
    "            # Per-competitor stats\n",
    "            summary_data.append(['Competitor', 'Mentions', 'Positive', 'Neutral', 'Negative'])\n",
    "            for comp in sorted(df['Competitor'].unique()):\n",
    "                comp_df = df[df['Competitor'] == comp]\n",
    "                pos = len(comp_df[comp_df['Sentiment_Label'] == 'positive'])\n",
    "                neu = len(comp_df[comp_df['Sentiment_Label'] == 'neutral'])\n",
    "                neg = len(comp_df[comp_df['Sentiment_Label'] == 'negative'])\n",
    "                summary_data.append([comp, len(comp_df), pos, neu, neg])\n",
    "            \n",
    "            # Write summary\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_excel(writer, sheet_name='Summary', index=False, header=False)\n",
    "            \n",
    "            # Format summary sheet\n",
    "            summary_ws = writer.sheets['Summary']\n",
    "            summary_ws.set_column('A:A', 20)\n",
    "            summary_ws.set_column('B:E', 12)\n",
    "    \n",
    "    print(f\"\\n‚úì Excel file saved: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "print(\"‚úì Excel export function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Excel\n",
    "excel_path = export_to_excel(\n",
    "    predictions_df,\n",
    "    'KFC_Predictions_Complete.xlsx',\n",
    "    include_summary=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Predictions exported to formatted Excel file!\")\n",
    "print(f\"  File: {excel_path}\")\n",
    "print(f\"  Sheets: 'Predictions' (main data), 'Summary' (statistics)\")\n",
    "print(f\"  Format: Color-coded sentiment, auto-width columns, summary stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save to Google Drive\n",
    "drive_path = f'{MODEL_SAVE_DIR}/KFC_Predictions_Complete.xlsx'\n",
    "predictions_df.to_excel(drive_path, index=False, engine='openpyxl')\n",
    "print(f\"\\n‚úì Also saved to Google Drive: {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING & PREDICTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Fixed Issues:\")\n",
    "print(\"  1. AdamW import (now uses torch.optim.AdamW)\")\n",
    "print(\"  2. CSV encoding (auto-converted to UTF-8)\")\n",
    "print(\"  3. Excel output (formatted .xlsx with color-coding)\")\n",
    "print(\"  4. NER model (single-label classification)\")\n",
    "\n",
    "print(\"\\nüìä Results:\")\n",
    "print(f\"  NER Best F1: {max(ner_history['val_f1']):.4f}\")\n",
    "print(f\"  Sentiment Best F1: {max(sentiment_history['val_f1']):.4f}\")\n",
    "print(f\"  Total predictions: {len(predictions_df)}\")\n",
    "print(f\"  From {len(df_test_pred_clean)} tweets\")\n",
    "\n",
    "print(\"\\nüìÅ Output Files:\")\n",
    "print(f\"  Excel (formatted): {excel_path}\")\n",
    "print(f\"  Google Drive: {drive_path}\")\n",
    "print(f\"  Models: {MODEL_SAVE_DIR}/\")\n",
    "print(f\"  Plots: {RESULTS_DIR}/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Ready to download results!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Excel file\n",
    "from google.colab import files\n",
    "files.download(excel_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
