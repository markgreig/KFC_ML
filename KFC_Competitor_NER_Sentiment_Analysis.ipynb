{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Multi-Competitor NER & Sentiment Analysis for Food Delivery Tweets\n",
    "\n",
    "**Objective**: Fine-tune deep learning models to:\n",
    "1. Extract competitor mentions from tweets (NER)\n",
    "2. Analyze sentiment for each competitor independently\n",
    "3. Handle multi-competitor tweets by generating separate predictions\n",
    "\n",
    "**Competitors**: Burger King, Deliveroo, Domino's, Five Guys, Greggs, Just Eat, KFC, McDonald's, Nando's, Papa John's, Pizza Hut, Pret a Manger, Taco Bell, Uber Eats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q transformers datasets torch torchvision accelerate\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install -q sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification, AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, f1_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and configure device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    gpu_memory_gb = gpu_props.total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_props.name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
    "    \n",
    "    # Adaptive batch size based on GPU memory\n",
    "    if gpu_memory_gb >= 15:\n",
    "        BATCH_SIZE = 16\n",
    "        GRAD_ACCUM_STEPS = 2\n",
    "    else:\n",
    "        BATCH_SIZE = 8\n",
    "        GRAD_ACCUM_STEPS = 4\n",
    "else:\n",
    "    BATCH_SIZE = 4\n",
    "    GRAD_ACCUM_STEPS = 8\n",
    "\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "print(f\"\\nBatch Configuration:\")\n",
    "print(f\"  Physical batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation steps: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"  Effective batch size: {EFFECTIVE_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive for data persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directory for saving models\n",
    "!mkdir -p '/content/drive/MyDrive/KFC_ML_Models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "constants"
   },
   "source": [
    "## 2. Constants & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# Competitor list\n",
    "COMPETITORS = [\n",
    "    'Burger King', 'Deliveroo', \"Domino's\", 'Five Guys', 'Greggs',\n",
    "    'Just Eat', 'KFC', \"McDonald's\", \"Nando's\", \"Papa John's\",\n",
    "    'Pizza Hut', 'Pret a Manger', 'Taco Bell', 'Uber Eats'\n",
    "]\n",
    "\n",
    "# Sentiment mapping\n",
    "SENTIMENT_MAP = {\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive'\n",
    "}\n",
    "\n",
    "# Model configurations\n",
    "NER_MODEL_NAME = 'bert-base-cased'\n",
    "SENTIMENT_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_SEQ_LENGTH = 128  # Sufficient for tweets\n",
    "NER_LEARNING_RATE = 3e-5\n",
    "SENTIMENT_LEARNING_RATE = 2e-5\n",
    "NER_EPOCHS = 4\n",
    "SENTIMENT_EPOCHS = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Paths\n",
    "MODEL_SAVE_DIR = '/content/drive/MyDrive/KFC_ML_Models'\n",
    "RESULTS_DIR = '/content/results'\n",
    "!mkdir -p {RESULTS_DIR}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 3. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "# Upload CSV files (if not already in Colab environment)\n",
    "# Option 1: Upload from local machine\n",
    "from google.colab import files\n",
    "print(\"Please upload the following CSV files:\")\n",
    "print(\"  1. KFC_social_data.xlsx - Sheet1.csv\")\n",
    "print(\"  2. KFC_training_sample.csv\")\n",
    "print(\"  3. KFC_test_sample.csv\")\n",
    "print(\"  4. KFC_test_sample_for_prediction.csv\")\n",
    "print(\"\\nClick 'Choose Files' below to upload...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Option 2: If files are in Google Drive, uncomment and modify path:\n",
    "# !cp '/content/drive/MyDrive/KFC_Data/*.csv' /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Large dataset\n",
    "df_large = pd.read_csv('KFC_social_data.xlsx - Sheet1.csv', low_memory=False)\n",
    "print(f\"Large dataset: {df_large.shape[0]} rows, {df_large.shape[1]} columns\")\n",
    "\n",
    "# Training sample\n",
    "df_train_sample = pd.read_csv('KFC_training_sample.csv')\n",
    "print(f\"Training sample: {df_train_sample.shape[0]} rows\")\n",
    "\n",
    "# Test with labels\n",
    "df_test = pd.read_csv('KFC_test_sample.csv')\n",
    "print(f\"Test sample (with labels): {df_test.shape[0]} rows\")\n",
    "\n",
    "# Test for prediction\n",
    "df_test_pred = pd.read_csv('KFC_test_sample_for_prediction.csv')\n",
    "print(f\"Test sample (for prediction): {df_test_pred.shape[0]} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore_large"
   },
   "outputs": [],
   "source": [
    "# Explore large dataset\n",
    "print(\"Large Dataset - First 3 rows:\")\n",
    "print(df_large.head(3))\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_large.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df_large.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_large.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore_distribution"
   },
   "outputs": [],
   "source": [
    "# Analyze competitor distribution\n",
    "print(\"Competitor Distribution in Large Dataset:\")\n",
    "competitor_counts = df_large['Competitor'].value_counts()\n",
    "print(competitor_counts)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "competitor_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Competitor Distribution in Dataset', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Competitor', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/competitor_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass imbalance detected - will use class weights during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_cleaning"
   },
   "source": [
    "## 4. Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clean_sentiment"
   },
   "outputs": [],
   "source": [
    "def clean_sentiment(value):\n",
    "    \"\"\"\n",
    "    Extract numeric sentiment (0, 1, 2) from messy SENTIMENT column.\n",
    "    Returns: 0 (negative), 1 (neutral), 2 (positive), or None if invalid\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    \n",
    "    # If already numeric\n",
    "    if isinstance(value, (int, float)):\n",
    "        if value in [0, 1, 2]:\n",
    "            return int(value)\n",
    "        return None\n",
    "    \n",
    "    # If string, try to extract\n",
    "    value_str = str(value).strip().lower()\n",
    "    \n",
    "    # Direct matches\n",
    "    if value_str in ['0', 'negative']:\n",
    "        return 0\n",
    "    elif value_str in ['1', 'neutral']:\n",
    "        return 1\n",
    "    elif value_str in ['2', 'positive']:\n",
    "        return 2\n",
    "    \n",
    "    # Try to find digit at start\n",
    "    match = re.match(r'^(\\d)', value_str)\n",
    "    if match:\n",
    "        digit = int(match.group(1))\n",
    "        if digit in [0, 1, 2]:\n",
    "            return digit\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test the function\n",
    "test_values = [0, 1, 2, '0', '1', '2', 'negative', 'positive', 'some text', None, 735.14]\n",
    "print(\"Testing clean_sentiment function:\")\n",
    "for val in test_values:\n",
    "    print(f\"  {val} -> {clean_sentiment(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "select_columns"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(df, name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Clean and prepare dataset:\n",
    "    - Keep essential columns\n",
    "    - Clean sentiment labels\n",
    "    - Handle missing values\n",
    "    - Clean tweet text\n",
    "    \"\"\"\n",
    "    print(f\"\\nPreparing {name}...\")\n",
    "    print(f\"  Initial rows: {len(df)}\")\n",
    "    \n",
    "    # Identify available columns\n",
    "    essential_cols = ['Competitor', 'Tweet', 'SENTIMENT']\n",
    "    metadata_cols = ['Impact', 'Impressions', 'Reach (new)']\n",
    "    multi_competitor_cols = ['KFC competitors', 'KFC competitors2', 'KFC competitors3']\n",
    "    \n",
    "    # Use Tweet column, or fallback to Title/Snippet/Full Text\n",
    "    if 'Tweet' not in df.columns:\n",
    "        if 'Full Text' in df.columns:\n",
    "            df['Tweet'] = df['Full Text']\n",
    "        elif 'Snippet' in df.columns:\n",
    "            df['Tweet'] = df['Snippet']\n",
    "        elif 'Title' in df.columns:\n",
    "            df['Tweet'] = df['Title']\n",
    "    \n",
    "    # Select available columns\n",
    "    cols_to_keep = [col for col in essential_cols if col in df.columns]\n",
    "    cols_to_keep += [col for col in metadata_cols if col in df.columns]\n",
    "    cols_to_keep += [col for col in multi_competitor_cols if col in df.columns]\n",
    "    \n",
    "    df_clean = df[cols_to_keep].copy()\n",
    "    \n",
    "    # Clean sentiment\n",
    "    if 'SENTIMENT' in df_clean.columns:\n",
    "        df_clean['SENTIMENT'] = df_clean['SENTIMENT'].apply(clean_sentiment)\n",
    "        before_drop = len(df_clean)\n",
    "        df_clean = df_clean.dropna(subset=['SENTIMENT'])\n",
    "        print(f\"  Dropped {before_drop - len(df_clean)} rows with invalid sentiment\")\n",
    "        df_clean['SENTIMENT'] = df_clean['SENTIMENT'].astype(int)\n",
    "    \n",
    "    # Drop rows with missing essential data\n",
    "    before_drop = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=['Competitor', 'Tweet'])\n",
    "    print(f\"  Dropped {before_drop - len(df_clean)} rows with missing Competitor/Tweet\")\n",
    "    \n",
    "    # Clean tweet text\n",
    "    df_clean['Tweet'] = df_clean['Tweet'].astype(str).str.strip()\n",
    "    df_clean = df_clean[df_clean['Tweet'].str.len() > 0]\n",
    "    \n",
    "    # Reset index\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  Final rows: {len(df_clean)}\")\n",
    "    print(f\"  Columns: {df_clean.columns.tolist()}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply to all datasets\n",
    "df_large_clean = prepare_dataset(df_large, \"Large dataset\")\n",
    "df_train_clean = prepare_dataset(df_train_sample, \"Training sample\")\n",
    "df_test_clean = prepare_dataset(df_test, \"Test sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_sentiment"
   },
   "outputs": [],
   "source": [
    "# Analyze sentiment distribution\n",
    "print(\"Sentiment Distribution in Large Dataset:\")\n",
    "sentiment_counts = df_large_clean['SENTIMENT'].value_counts().sort_index()\n",
    "for sent, count in sentiment_counts.items():\n",
    "    print(f\"  {sent} ({SENTIMENT_MAP[sent]}): {count} ({count/len(df_large_clean)*100:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_labels = [SENTIMENT_MAP[s] for s in sentiment_counts.index]\n",
    "axes[0].bar(sentiment_labels, sentiment_counts.values, color=['#ff6b6b', '#95a5a6', '#51cf66'], edgecolor='black')\n",
    "axes[0].set_title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Sentiment pie chart\n",
    "axes[1].pie(sentiment_counts.values, labels=sentiment_labels, autopct='%1.1f%%',\n",
    "            colors=['#ff6b6b', '#95a5a6', '#51cf66'], startangle=90)\n",
    "axes[1].set_title('Sentiment Proportions', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/sentiment_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "multi_competitor_analysis"
   },
   "outputs": [],
   "source": [
    "# Analyze multi-competitor tweets\n",
    "def count_competitors_in_row(row):\n",
    "    \"\"\"Count how many competitors are mentioned in a row\"\"\"\n",
    "    count = 1  # Primary competitor\n",
    "    if 'KFC competitors' in row and pd.notna(row['KFC competitors']) and str(row['KFC competitors']).strip():\n",
    "        count += 1\n",
    "    if 'KFC competitors2' in row and pd.notna(row['KFC competitors2']) and str(row['KFC competitors2']).strip():\n",
    "        count += 1\n",
    "    if 'KFC competitors3' in row and pd.notna(row['KFC competitors3']) and str(row['KFC competitors3']).strip():\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "if 'KFC competitors' in df_large_clean.columns:\n",
    "    df_large_clean['num_competitors'] = df_large_clean.apply(count_competitors_in_row, axis=1)\n",
    "    multi_comp_counts = df_large_clean['num_competitors'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\nMulti-Competitor Tweet Analysis:\")\n",
    "    for num, count in multi_comp_counts.items():\n",
    "        print(f\"  {num} competitor(s): {count} tweets ({count/len(df_large_clean)*100:.1f}%)\")\n",
    "    \n",
    "    # Show example of multi-competitor tweet\n",
    "    multi_comp_example = df_large_clean[df_large_clean['num_competitors'] > 1].iloc[0]\n",
    "    print(\"\\nExample multi-competitor tweet:\")\n",
    "    print(f\"  Primary: {multi_comp_example['Competitor']}\")\n",
    "    if pd.notna(multi_comp_example.get('KFC competitors')):\n",
    "        print(f\"  Also mentions: {multi_comp_example['KFC competitors']}\")\n",
    "    print(f\"  Tweet: {multi_comp_example['Tweet'][:150]}...\")\n",
    "else:\n",
    "    print(\"\\nNo multi-competitor columns found in this dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "expand_multi_competitor"
   },
   "outputs": [],
   "source": [
    "def expand_multi_competitor_tweets(df):\n",
    "    \"\"\"\n",
    "    Expand tweets with multiple competitors into separate rows.\n",
    "    Each row will have (Tweet, Competitor, Sentiment, Metadata)\n",
    "    \n",
    "    Note: For training, we assume same sentiment applies to all mentioned competitors.\n",
    "    The model will learn to differentiate based on context.\n",
    "    \"\"\"\n",
    "    expanded_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Expanding multi-competitor tweets\"):\n",
    "        # Primary competitor\n",
    "        expanded_rows.append(row.to_dict())\n",
    "        \n",
    "        # Additional competitors (if columns exist)\n",
    "        for comp_col in ['KFC competitors', 'KFC competitors2', 'KFC competitors3']:\n",
    "            if comp_col in row and pd.notna(row[comp_col]) and str(row[comp_col]).strip():\n",
    "                new_row = row.to_dict().copy()\n",
    "                new_row['Competitor'] = str(row[comp_col]).strip()\n",
    "                expanded_rows.append(new_row)\n",
    "    \n",
    "    df_expanded = pd.DataFrame(expanded_rows)\n",
    "    \n",
    "    # Drop the multi-competitor columns\n",
    "    cols_to_drop = [col for col in ['KFC competitors', 'KFC competitors2', 'KFC competitors3', 'num_competitors'] \n",
    "                    if col in df_expanded.columns]\n",
    "    df_expanded = df_expanded.drop(columns=cols_to_drop)\n",
    "    \n",
    "    return df_expanded.reset_index(drop=True)\n",
    "\n",
    "# Expand large dataset\n",
    "print(f\"Before expansion: {len(df_large_clean)} rows\")\n",
    "df_expanded = expand_multi_competitor_tweets(df_large_clean)\n",
    "print(f\"After expansion: {len(df_expanded)} rows\")\n",
    "print(f\"Added {len(df_expanded) - len(df_large_clean)} additional competitor-tweet pairs\")\n",
    "\n",
    "# Show updated competitor distribution\n",
    "print(\"\\nUpdated Competitor Distribution:\")\n",
    "print(df_expanded['Competitor'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baseline"
   },
   "source": [
    "## 5. Rule-Based Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baseline_ner"
   },
   "outputs": [],
   "source": [
    "def baseline_ner(tweet_text):\n",
    "    \"\"\"\n",
    "    Simple rule-based competitor extraction using regex.\n",
    "    Returns list of competitors found in tweet.\n",
    "    \"\"\"\n",
    "    found_competitors = []\n",
    "    tweet_lower = tweet_text.lower()\n",
    "    \n",
    "    # Check for each competitor (case-insensitive)\n",
    "    for competitor in COMPETITORS:\n",
    "        # Handle variations\n",
    "        patterns = [competitor.lower()]\n",
    "        \n",
    "        # Add common variations\n",
    "        if competitor == \"McDonald's\":\n",
    "            patterns.extend(['mcdonald', 'mcdonalds', 'maccies', 'maccas', 'mickey d'])\n",
    "        elif competitor == \"KFC\":\n",
    "            patterns.extend(['kentucky fried chicken', 'kentucky'])\n",
    "        elif competitor == \"Nando's\":\n",
    "            patterns.extend(['nando', 'nandos'])\n",
    "        elif competitor == \"Papa John's\":\n",
    "            patterns.extend(['papa john', 'papa johns'])\n",
    "        elif competitor == \"Domino's\":\n",
    "            patterns.extend(['domino', 'dominos'])\n",
    "        elif competitor == \"Pret a Manger\":\n",
    "            patterns.extend(['pret'])\n",
    "        \n",
    "        # Check if any pattern matches\n",
    "        for pattern in patterns:\n",
    "            if re.search(r'\\b' + re.escape(pattern) + r'\\b', tweet_lower):\n",
    "                found_competitors.append(competitor)\n",
    "                break\n",
    "    \n",
    "    return found_competitors\n",
    "\n",
    "# Test baseline NER\n",
    "test_tweets = [\n",
    "    \"I love KFC's chicken!\",\n",
    "    \"McDonald's fries are better than Burger King\",\n",
    "    \"Just ordered from Deliveroo\",\n",
    "    \"This is a random tweet with no competitors\"\n",
    "]\n",
    "\n",
    "print(\"Testing Baseline NER:\")\n",
    "for tweet in test_tweets:\n",
    "    competitors = baseline_ner(tweet)\n",
    "    print(f\"  Tweet: {tweet}\")\n",
    "    print(f\"  Found: {competitors}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baseline_sentiment"
   },
   "outputs": [],
   "source": [
    "def baseline_sentiment(tweet_text):\n",
    "    \"\"\"\n",
    "    Simple lexicon-based sentiment analysis.\n",
    "    Returns: 0 (negative), 1 (neutral), 2 (positive)\n",
    "    \"\"\"\n",
    "    # Simple positive/negative word lists\n",
    "    positive_words = [\n",
    "        'good', 'great', 'excellent', 'amazing', 'awesome', 'love', 'best', 'perfect',\n",
    "        'delicious', 'tasty', 'yummy', 'fantastic', 'wonderful', 'brilliant', 'nice',\n",
    "        'happy', 'satisfied', 'enjoyed', 'recommend', 'favorite', 'favourite'\n",
    "    ]\n",
    "    \n",
    "    negative_words = [\n",
    "        'bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'poor', 'disgusting',\n",
    "        'nasty', 'gross', 'disappointing', 'disappointed', 'never', 'rubbish', 'trash',\n",
    "        'cold', 'overpriced', 'expensive', 'slow', 'rude', 'complaint'\n",
    "    ]\n",
    "    \n",
    "    tweet_lower = tweet_text.lower()\n",
    "    \n",
    "    # Count positive and negative words\n",
    "    pos_count = sum(1 for word in positive_words if re.search(r'\\b' + word + r'\\b', tweet_lower))\n",
    "    neg_count = sum(1 for word in negative_words if re.search(r'\\b' + word + r'\\b', tweet_lower))\n",
    "    \n",
    "    # Simple logic\n",
    "    if pos_count > neg_count:\n",
    "        return 2  # positive\n",
    "    elif neg_count > pos_count:\n",
    "        return 0  # negative\n",
    "    else:\n",
    "        return 1  # neutral\n",
    "\n",
    "# Test baseline sentiment\n",
    "test_tweets = [\n",
    "    \"I love this place! The food is amazing!\",\n",
    "    \"This is terrible. Worst experience ever.\",\n",
    "    \"I went to the store today.\",\n",
    "    \"Great chicken but terrible service\"\n",
    "]\n",
    "\n",
    "print(\"Testing Baseline Sentiment:\")\n",
    "for tweet in test_tweets:\n",
    "    sentiment = baseline_sentiment(tweet)\n",
    "    print(f\"  Tweet: {tweet}\")\n",
    "    print(f\"  Sentiment: {sentiment} ({SENTIMENT_MAP[sentiment]})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_baseline"
   },
   "outputs": [],
   "source": [
    "# Evaluate baseline on test set\n",
    "def evaluate_baseline(df_test):\n",
    "    \"\"\"\n",
    "    Evaluate baseline model on test set.\n",
    "    \"\"\"\n",
    "    print(\"Evaluating Baseline Model on Test Set...\\n\")\n",
    "    \n",
    "    # NER evaluation\n",
    "    ner_correct = 0\n",
    "    ner_total = len(df_test)\n",
    "    \n",
    "    # Sentiment evaluation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for idx, row in df_test.iterrows():\n",
    "        tweet = row['Tweet']\n",
    "        true_competitor = row['Competitor']\n",
    "        true_sentiment = row['SENTIMENT']\n",
    "        \n",
    "        # NER: Check if baseline finds the correct competitor\n",
    "        found_competitors = baseline_ner(tweet)\n",
    "        if true_competitor in found_competitors:\n",
    "            ner_correct += 1\n",
    "        \n",
    "        # Sentiment\n",
    "        pred_sentiment = baseline_sentiment(tweet)\n",
    "        y_true.append(true_sentiment)\n",
    "        y_pred.append(pred_sentiment)\n",
    "    \n",
    "    # NER metrics\n",
    "    ner_accuracy = ner_correct / ner_total\n",
    "    print(f\"Baseline NER Accuracy: {ner_accuracy:.3f}\")\n",
    "    print(f\"  Correctly identified competitor in {ner_correct}/{ner_total} tweets\\n\")\n",
    "    \n",
    "    # Sentiment metrics\n",
    "    sentiment_accuracy = accuracy_score(y_true, y_pred)\n",
    "    sentiment_f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"Baseline Sentiment Accuracy: {sentiment_accuracy:.3f}\")\n",
    "    print(f\"Baseline Sentiment F1 (macro): {sentiment_f1_macro:.3f}\\n\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                                target_names=['negative', 'neutral', 'positive'],\n",
    "                                zero_division=0))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['negative', 'neutral', 'positive'],\n",
    "                yticklabels=['negative', 'neutral', 'positive'])\n",
    "    plt.title('Baseline Model - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/baseline_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'ner_accuracy': ner_accuracy,\n",
    "        'sentiment_accuracy': sentiment_accuracy,\n",
    "        'sentiment_f1_macro': sentiment_f1_macro\n",
    "    }\n",
    "\n",
    "# Run baseline evaluation\n",
    "baseline_results = evaluate_baseline(df_test_clean)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Baseline model establishes our performance floor.\")\n",
    "print(\"Deep learning models should significantly outperform these metrics.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare_training"
   },
   "source": [
    "## 6. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stratified_split"
   },
   "outputs": [],
   "source": [
    "# Create stratified train/validation split\n",
    "# Stratify by both competitor and sentiment to ensure balanced representation\n",
    "\n",
    "# Create stratification key\n",
    "df_expanded['stratify_key'] = df_expanded['Competitor'] + '_' + df_expanded['SENTIMENT'].astype(str)\n",
    "\n",
    "# Check if we have enough samples for each combination\n",
    "stratify_counts = df_expanded['stratify_key'].value_counts()\n",
    "print(f\"Number of unique Competitor-Sentiment combinations: {len(stratify_counts)}\")\n",
    "print(f\"Minimum samples in a combination: {stratify_counts.min()}\")\n",
    "\n",
    "# For combinations with only 1 sample, we can't stratify perfectly\n",
    "# Use a more robust approach\n",
    "rare_combinations = stratify_counts[stratify_counts < 2].index\n",
    "if len(rare_combinations) > 0:\n",
    "    print(f\"\\nWarning: {len(rare_combinations)} combinations have <2 samples\")\n",
    "    print(\"Using competitor-only stratification for robustness\")\n",
    "    stratify_column = df_expanded['Competitor']\n",
    "else:\n",
    "    stratify_column = df_expanded['stratify_key']\n",
    "\n",
    "# Split data (80/20)\n",
    "train_df, val_df = train_test_split(\n",
    "    df_expanded,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=stratify_column\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"  Training: {len(train_df)} samples\")\n",
    "print(f\"  Validation: {len(val_df)} samples\")\n",
    "print(f\"  Test: {len(df_test_clean)} samples\")\n",
    "\n",
    "# Verify distribution is preserved\n",
    "print(\"\\nCompetitor distribution in splits:\")\n",
    "print(\"Training:\")\n",
    "print(train_df['Competitor'].value_counts(normalize=True).head())\n",
    "print(\"\\nValidation:\")\n",
    "print(val_df['Competitor'].value_counts(normalize=True).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ner_model_section"
   },
   "source": [
    "## 7. NER Model - Competitor Extraction\n",
    "\n",
    "We'll use a multi-label classification approach:\n",
    "- Input: Tweet text\n",
    "- Output: Binary vector indicating which competitors are mentioned (14 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ner_dataset_class"
   },
   "outputs": [],
   "source": [
    "class CompetitorNERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for multi-label competitor identification.\n",
    "    Each sample is labeled with which competitors are mentioned.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Group by tweet to identify all competitors in each tweet\n",
    "        self.tweet_groups = self.data.groupby('Tweet')['Competitor'].apply(list).to_dict()\n",
    "        self.unique_tweets = list(self.tweet_groups.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.unique_tweets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.unique_tweets[idx]\n",
    "        competitors_in_tweet = self.tweet_groups[tweet]\n",
    "        \n",
    "        # Tokenize tweet\n",
    "        encoding = self.tokenizer(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Create multi-label target (binary vector for each competitor)\n",
    "        labels = torch.zeros(len(COMPETITORS), dtype=torch.float)\n",
    "        for competitor in competitors_in_tweet:\n",
    "            if competitor in COMPETITORS:\n",
    "                labels[COMPETITORS.index(competitor)] = 1.0\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "print(\"CompetitorNERDataset class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ner_model_class"
   },
   "outputs": [],
   "source": [
    "class MultiLabelCompetitorModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-label classifier for competitor identification.\n",
    "    Uses BERT with sigmoid activation for independent binary predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "print(\"MultiLabelCompetitorModel class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ner_prepare_data"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer for NER model\n",
    "print(f\"Loading tokenizer: {NER_MODEL_NAME}\")\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating NER datasets...\")\n",
    "ner_train_dataset = CompetitorNERDataset(train_df, ner_tokenizer, MAX_SEQ_LENGTH)\n",
    "ner_val_dataset = CompetitorNERDataset(val_df, ner_tokenizer, MAX_SEQ_LENGTH)\n",
    "ner_test_dataset = CompetitorNERDataset(df_test_clean, ner_tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "print(f\"  Train: {len(ner_train_dataset)} unique tweets\")\n",
    "print(f\"  Validation: {len(ner_val_dataset)} unique tweets\")\n",
    "print(f\"  Test: {len(ner_test_dataset)} unique tweets\")\n",
    "\n",
    "# Create dataloaders\n",
    "ner_train_loader = DataLoader(\n",
    "    ner_train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "ner_val_loader = DataLoader(\n",
    "    ner_val_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,  # Can use larger batch for inference\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "ner_test_loader = DataLoader(\n",
    "    ner_test_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch size {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ner_train_function"
   },
   "outputs": [],
   "source": [
    "def train_ner_model(model, train_loader, val_loader, epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train the NER (multi-label competitor identification) model.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs // GRAD_ACCUM_STEPS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    print(f\"\\nStarting NER model training...\")\n",
    "    print(f\"  Total epochs: {epochs}\")\n",
    "    print(f\"  Total steps: {total_steps}\")\n",
    "    print(f\"  Warmup steps: {warmup_steps}\")\n",
    "    print(f\"  Gradient accumulation: {GRAD_ACCUM_STEPS}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for step, batch in enumerate(train_pbar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask, labels)\n",
    "                loss = outputs.loss / GRAD_ACCUM_STEPS\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Update weights every GRAD_ACCUM_STEPS\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item() * GRAD_ACCUM_STEPS:.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(input_ids, attention_mask, labels)\n",
    "                \n",
    "                val_loss += outputs.loss.item()\n",
    "                \n",
    "                # Get predictions (threshold at 0.5)\n",
    "                logits = outputs.logits\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "                \n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_labels = torch.cat(all_labels).numpy()\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            print(f\"  New best F1! Saving model...\")\n",
    "            torch.save(model.state_dict(), f'{MODEL_SAVE_DIR}/ner_best_model.pt')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"Best validation F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Train function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ner_train"
   },
   "outputs": [],
   "source": [
    "# Initialize and train NER model\n",
    "print(f\"Initializing NER model: {NER_MODEL_NAME}\")\n",
    "ner_model = MultiLabelCompetitorModel(NER_MODEL_NAME, num_labels=len(COMPETITORS))\n",
    "\n",
    "# Train model\n",
    "ner_model, ner_history = train_ner_model(\n",
    "    ner_model,\n",
    "    ner_train_loader,\n",
    "    ner_val_loader,\n",
    "    epochs=NER_EPOCHS,\n",
    "    learning_rate=NER_LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ner_plot_history"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(ner_history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(ner_history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_title('NER Model - Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[1].plot(ner_history['val_f1'], label='Val F1 (macro)', marker='o', color='green')\n",
    "axes[1].set_title('NER Model - Validation F1 Score', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/ner_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ner_evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate NER model on test set\n",
    "def evaluate_ner_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate NER model and return detailed metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating NER\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    overall_precision = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)[0]\n",
    "    overall_recall = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)[1]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"NER Model Test Results\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Overall Precision: {overall_precision:.4f}\")\n",
    "    print(f\"Overall Recall: {overall_recall:.4f}\")\n",
    "    print(f\"Overall F1 (macro): {overall_f1:.4f}\")\n",
    "    \n",
    "    # Per-competitor metrics\n",
    "    print(\"\\nPer-Competitor Performance:\")\n",
    "    print(\"-\" * 50)\n",
    "    per_competitor_f1 = []\n",
    "    for i, competitor in enumerate(COMPETITORS):\n",
    "        if all_labels[:, i].sum() > 0:  # Only if competitor appears in test set\n",
    "            f1 = f1_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n",
    "            per_competitor_f1.append((competitor, f1))\n",
    "            print(f\"  {competitor:20s}: F1 = {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'overall_f1': overall_f1,\n",
    "        'overall_precision': overall_precision,\n",
    "        'overall_recall': overall_recall,\n",
    "        'per_competitor_f1': per_competitor_f1\n",
    "    }\n",
    "\n",
    "# Load best model and evaluate\n",
    "ner_model.load_state_dict(torch.load(f'{MODEL_SAVE_DIR}/ner_best_model.pt'))\n",
    "ner_test_results = evaluate_ner_model(ner_model, ner_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sentiment Model - Competitor-Specific Sentiment Classification\n",
    "\n",
    "We'll fine-tune a Twitter-specific RoBERTa model for sentiment classification.\n",
    "Input format: `\"[Tweet text] This tweet is about [Competitor name].\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for competitor-specific sentiment classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        tweet = row['Tweet']\n",
    "        competitor = row['Competitor']\n",
    "        sentiment = row['SENTIMENT']\n",
    "        \n",
    "        # Create contextualized input\n",
    "        text = f\"{tweet} This tweet is about {competitor}.\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(sentiment, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"SentimentDataset class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer for sentiment model\n",
    "print(f\"Loading tokenizer: {SENTIMENT_MODEL_NAME}\")\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(SENTIMENT_MODEL_NAME)\n",
    "\n",
    "# Create datasets (using expanded data with competitor-tweet pairs)\n",
    "print(\"\\nCreating Sentiment datasets...\")\n",
    "sentiment_train_dataset = SentimentDataset(train_df, sentiment_tokenizer, MAX_SEQ_LENGTH)\n",
    "sentiment_val_dataset = SentimentDataset(val_df, sentiment_tokenizer, MAX_SEQ_LENGTH)\n",
    "sentiment_test_dataset = SentimentDataset(df_test_clean, sentiment_tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "print(f\"  Train: {len(sentiment_train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(sentiment_val_dataset)} samples\")\n",
    "print(f\"  Test: {len(sentiment_test_dataset)} samples\")\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "train_sentiments = train_df['SENTIMENT'].values\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_sentiments),\n",
    "    y=train_sentiments\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"\\nClass weights for imbalanced sentiment:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"  {SENTIMENT_MAP[i]:8s}: {weight:.3f}\")\n",
    "\n",
    "# Create dataloaders\n",
    "sentiment_train_loader = DataLoader(\n",
    "    sentiment_train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "sentiment_val_loader = DataLoader(\n",
    "    sentiment_val_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "sentiment_test_loader = DataLoader(\n",
    "    sentiment_test_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch size {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentiment_model(model, train_loader, val_loader, epochs, learning_rate, class_weights):\n",
    "    \"\"\"\n",
    "    Train the sentiment classification model with class weights.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs // GRAD_ACCUM_STEPS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss function with class weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    print(f\"\\nStarting Sentiment model training...\")\n",
    "    print(f\"  Total epochs: {epochs}\")\n",
    "    print(f\"  Total steps: {total_steps}\")\n",
    "    print(f\"  Warmup steps: {warmup_steps}\")\n",
    "    print(f\"  Gradient accumulation: {GRAD_ACCUM_STEPS}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for step, batch in enumerate(train_pbar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs.logits, labels) / GRAD_ACCUM_STEPS\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Update weights every GRAD_ACCUM_STEPS\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item() * GRAD_ACCUM_STEPS:.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            print(f\"  New best F1! Saving model...\")\n",
    "            torch.save(model.state_dict(), f'{MODEL_SAVE_DIR}/sentiment_best_model.pt')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"Best validation F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Sentiment train function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train sentiment model\n",
    "print(f\"Initializing Sentiment model: {SENTIMENT_MODEL_NAME}\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    SENTIMENT_MODEL_NAME,\n",
    "    num_labels=3,  # negative, neutral, positive\n",
    "    ignore_mismatched_sizes=True  # In case pre-trained model has different num_labels\n",
    ")\n",
    "\n",
    "# Train model\n",
    "sentiment_model, sentiment_history = train_sentiment_model(\n",
    "    sentiment_model,\n",
    "    sentiment_train_loader,\n",
    "    sentiment_val_loader,\n",
    "    epochs=SENTIMENT_EPOCHS,\n",
    "    learning_rate=SENTIMENT_LEARNING_RATE,\n",
    "    class_weights=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(sentiment_history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(sentiment_history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_title('Sentiment Model - Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(sentiment_history['val_accuracy'], label='Val Accuracy', marker='o', color='blue')\n",
    "axes[1].set_title('Sentiment Model - Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[2].plot(sentiment_history['val_f1'], label='Val F1 (macro)', marker='o', color='green')\n",
    "axes[2].set_title('Sentiment Model - Validation F1 Score', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/sentiment_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate sentiment model on test set\n",
    "def evaluate_sentiment_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate sentiment model and return detailed metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating Sentiment\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Sentiment Model Test Results\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
    "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        all_labels, all_preds,\n",
    "        target_names=['negative', 'neutral', 'positive'],\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['negative', 'neutral', 'positive'],\n",
    "                yticklabels=['negative', 'neutral', 'positive'])\n",
    "    plt.title('Sentiment Model - Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/sentiment_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "# Load best model and evaluate\n",
    "sentiment_model.load_state_dict(torch.load(f'{MODEL_SAVE_DIR}/sentiment_best_model.pt'))\n",
    "sentiment_test_results = evaluate_sentiment_model(sentiment_model, sentiment_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integrated Pipeline - End-to-End Inference\n",
    "\n",
    "Combine NER and Sentiment models to:\n",
    "1. Extract all competitors from tweet\n",
    "2. Predict sentiment for each competitor\n",
    "3. Generate output with one row per competitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_competitors_and_sentiment(tweet_text, ner_model, sentiment_model, \n",
    "                                      ner_tokenizer, sentiment_tokenizer,\n",
    "                                      threshold=0.5):\n",
    "    \"\"\"\n",
    "    End-to-end prediction:\n",
    "    1. Extract competitors using NER model\n",
    "    2. For each competitor, predict sentiment\n",
    "    \n",
    "    Returns: List of (competitor, sentiment) tuples\n",
    "    \"\"\"\n",
    "    ner_model.eval()\n",
    "    sentiment_model.eval()\n",
    "    \n",
    "    # Step 1: Extract competitors\n",
    "    ner_encoding = ner_tokenizer(\n",
    "        tweet_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            ner_outputs = ner_model(\n",
    "                ner_encoding['input_ids'].to(device),\n",
    "                ner_encoding['attention_mask'].to(device)\n",
    "            )\n",
    "    \n",
    "    # Get competitor predictions (apply sigmoid and threshold)\n",
    "    ner_probs = torch.sigmoid(ner_outputs.logits).cpu().numpy()[0]\n",
    "    detected_competitors = [COMPETITORS[i] for i, prob in enumerate(ner_probs) if prob > threshold]\n",
    "    \n",
    "    if len(detected_competitors) == 0:\n",
    "        # Fallback: use baseline NER if model finds nothing\n",
    "        detected_competitors = baseline_ner(tweet_text)\n",
    "    \n",
    "    # Step 2: For each competitor, predict sentiment\n",
    "    results = []\n",
    "    \n",
    "    for competitor in detected_competitors:\n",
    "        # Create contextualized input\n",
    "        text = f\"{tweet_text} This tweet is about {competitor}.\"\n",
    "        \n",
    "        sentiment_encoding = sentiment_tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                sentiment_outputs = sentiment_model(\n",
    "                    sentiment_encoding['input_ids'].to(device),\n",
    "                    sentiment_encoding['attention_mask'].to(device)\n",
    "                )\n",
    "        \n",
    "        predicted_sentiment = torch.argmax(sentiment_outputs.logits, dim=1).item()\n",
    "        results.append((competitor, predicted_sentiment))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the pipeline\n",
    "test_tweets = [\n",
    "    \"I love KFC's chicken!\",\n",
    "    \"McDonald's fries are amazing but their burgers are terrible\",\n",
    "    \"Just ordered from Deliveroo and the food was cold\",\n",
    "]\n",
    "\n",
    "print(\"Testing Integrated Pipeline:\\n\")\n",
    "for tweet in test_tweets:\n",
    "    results = predict_competitors_and_sentiment(\n",
    "        tweet, ner_model, sentiment_model,\n",
    "        ner_tokenizer, sentiment_tokenizer\n",
    "    )\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    for competitor, sentiment in results:\n",
    "        print(f\"  {competitor}: {SENTIMENT_MAP[sentiment]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_pipeline(df, ner_model, sentiment_model, \n",
    "                                  ner_tokenizer, sentiment_tokenizer):\n",
    "    \"\"\"\n",
    "    Process entire dataset through the pipeline.\n",
    "    Returns DataFrame with one row per detected competitor.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing tweets\"):\n",
    "        tweet = row['Tweet']\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = predict_competitors_and_sentiment(\n",
    "            tweet, ner_model, sentiment_model,\n",
    "            ner_tokenizer, sentiment_tokenizer\n",
    "        )\n",
    "        \n",
    "        # Create row for each detected competitor\n",
    "        for competitor, sentiment in predictions:\n",
    "            result_row = {\n",
    "                'Competitor': competitor,\n",
    "                'Tweet': tweet,\n",
    "                'Predicted_Sentiment': sentiment,\n",
    "                'Sentiment_Label': SENTIMENT_MAP[sentiment]\n",
    "            }\n",
    "            \n",
    "            # Add metadata if available\n",
    "            for col in ['Impact', 'Impressions', 'Reach (new)']:\n",
    "                if col in row:\n",
    "                    result_row[col] = row[col]\n",
    "            \n",
    "            results.append(result_row)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Pipeline processing function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Predictions for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test data for prediction\n",
    "print(\"Generating predictions for KFC_test_sample_for_prediction.csv...\\n\")\n",
    "\n",
    "# Prepare test prediction data (might not have SENTIMENT column)\n",
    "if 'SENTIMENT' not in df_test_pred.columns:\n",
    "    df_test_pred['SENTIMENT'] = 1  # Dummy value for processing\n",
    "\n",
    "df_test_pred_clean = prepare_dataset(df_test_pred, \"Test prediction data\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions_df = process_dataset_with_pipeline(\n",
    "    df_test_pred_clean,\n",
    "    ner_model,\n",
    "    sentiment_model,\n",
    "    ner_tokenizer,\n",
    "    sentiment_tokenizer\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions_df)} predictions from {len(df_test_pred_clean)} tweets\")\n",
    "print(f\"Average competitors per tweet: {len(predictions_df) / len(df_test_pred_clean):.2f}\")\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(predictions_df.head(10))\n",
    "\n",
    "# Save predictions\n",
    "predictions_output_path = f'{RESULTS_DIR}/KFC_test_predictions.csv'\n",
    "predictions_df.to_csv(predictions_output_path, index=False)\n",
    "print(f\"\\nPredictions saved to: {predictions_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs. fine-tuned models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. NER (Competitor Extraction) Performance:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Baseline NER Accuracy:        {baseline_results['ner_accuracy']:.4f}\")\n",
    "print(f\"Fine-tuned NER F1 (macro):    {ner_test_results['overall_f1']:.4f}\")\n",
    "print(f\"Improvement:                  {(ner_test_results['overall_f1'] - baseline_results['ner_accuracy']):.4f}\")\n",
    "\n",
    "print(\"\\n2. Sentiment Classification Performance:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Baseline Sentiment Accuracy:  {baseline_results['sentiment_accuracy']:.4f}\")\n",
    "print(f\"Fine-tuned Sentiment Accuracy: {sentiment_test_results['accuracy']:.4f}\")\n",
    "print(f\"Improvement:                  {(sentiment_test_results['accuracy'] - baseline_results['sentiment_accuracy']):.4f}\")\n",
    "\n",
    "print(f\"\\nBaseline Sentiment F1:        {baseline_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"Fine-tuned Sentiment F1:      {sentiment_test_results['f1_macro']:.4f}\")\n",
    "print(f\"Improvement:                  {(sentiment_test_results['f1_macro'] - baseline_results['sentiment_f1_macro']):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Models successfully fine-tuned and evaluated!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# NER comparison\n",
    "ner_methods = ['Baseline', 'Fine-tuned']\n",
    "ner_scores = [baseline_results['ner_accuracy'], ner_test_results['overall_f1']]\n",
    "axes[0].bar(ner_methods, ner_scores, color=['#ff7f0e', '#2ca02c'], edgecolor='black')\n",
    "axes[0].set_title('NER Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, score in enumerate(ner_scores):\n",
    "    axes[0].text(i, score + 0.02, f'{score:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Sentiment comparison\n",
    "sentiment_methods = ['Baseline', 'Fine-tuned']\n",
    "sentiment_scores = [baseline_results['sentiment_f1_macro'], sentiment_test_results['f1_macro']]\n",
    "axes[1].bar(sentiment_methods, sentiment_scores, color=['#ff7f0e', '#2ca02c'], edgecolor='black')\n",
    "axes[1].set_title('Sentiment F1 Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('F1 Score (macro)', fontsize=12)\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, score in enumerate(sentiment_scores):\n",
    "    axes[1].text(i, score + 0.02, f'{score:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Models to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizers and final configuration\n",
    "print(\"Saving models and tokenizers to Google Drive...\\n\")\n",
    "\n",
    "# Save tokenizers\n",
    "ner_tokenizer.save_pretrained(f'{MODEL_SAVE_DIR}/ner_tokenizer')\n",
    "sentiment_tokenizer.save_pretrained(f'{MODEL_SAVE_DIR}/sentiment_tokenizer')\n",
    "\n",
    "# Save full models (optional - files will be large)\n",
    "ner_model.bert.save_pretrained(f'{MODEL_SAVE_DIR}/ner_full_model')\n",
    "sentiment_model.save_pretrained(f'{MODEL_SAVE_DIR}/sentiment_full_model')\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'competitors': COMPETITORS,\n",
    "    'sentiment_map': SENTIMENT_MAP,\n",
    "    'ner_model_name': NER_MODEL_NAME,\n",
    "    'sentiment_model_name': SENTIMENT_MODEL_NAME,\n",
    "    'max_seq_length': MAX_SEQ_LENGTH,\n",
    "    'ner_epochs': NER_EPOCHS,\n",
    "    'sentiment_epochs': SENTIMENT_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'baseline_results': baseline_results,\n",
    "    'ner_test_results': ner_test_results,\n",
    "    'sentiment_test_results': {\n",
    "        'accuracy': sentiment_test_results['accuracy'],\n",
    "        'f1_macro': sentiment_test_results['f1_macro'],\n",
    "        'f1_weighted': sentiment_test_results['f1_weighted']\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{MODEL_SAVE_DIR}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\" Models and tokenizers saved successfully!\")\n",
    "print(f\"\\nSaved to: {MODEL_SAVE_DIR}/\")\n",
    "print(\"  - ner_best_model.pt\")\n",
    "print(\"  - sentiment_best_model.pt\")\n",
    "print(\"  - ner_tokenizer/\")\n",
    "print(\"  - sentiment_tokenizer/\")\n",
    "print(\"  - ner_full_model/\")\n",
    "print(\"  - sentiment_full_model/\")\n",
    "print(\"  - config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Dataset Statistics:\")\n",
    "print(f\"  Training samples: {len(train_df)}\")\n",
    "print(f\"  Validation samples: {len(val_df)}\")\n",
    "print(f\"  Test samples: {len(df_test_clean)}\")\n",
    "print(f\"  Total competitors: {len(COMPETITORS)}\")\n",
    "\n",
    "print(\"\\n Model Performance:\")\n",
    "print(f\"  NER F1 Score: {ner_test_results['overall_f1']:.4f}\")\n",
    "print(f\"  Sentiment Accuracy: {sentiment_test_results['accuracy']:.4f}\")\n",
    "print(f\"  Sentiment F1 (macro): {sentiment_test_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(\"\\n Output Files:\")\n",
    "print(f\"  Predictions: {RESULTS_DIR}/KFC_test_predictions.csv\")\n",
    "print(f\"  Models: {MODEL_SAVE_DIR}/\")\n",
    "print(f\"  Visualizations: {RESULTS_DIR}/\")\n",
    "\n",
    "print(\"\\n Key Features:\")\n",
    "print(\"   Multi-competitor extraction\")\n",
    "print(\"   Competitor-specific sentiment analysis\")\n",
    "print(\"   Handles tweets mentioning multiple competitors\")\n",
    "print(\"   Class-weighted training for imbalanced data\")\n",
    "print(\"   Mixed precision training for efficiency\")\n",
    "\n",
    "print(\"\\n Next Steps:\")\n",
    "print(\"  1. Review predictions in KFC_test_predictions.csv\")\n",
    "print(\"  2. Analyze error cases to identify improvement areas\")\n",
    "print(\"  3. Consider data augmentation for underrepresented competitors\")\n",
    "print(\"  4. Fine-tune threshold for NER (currently 0.5)\")\n",
    "print(\"  5. Deploy models for real-time inference\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Thank you for using this pipeline!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
