{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multi-Competitor NER & Sentiment Analysis - IMPROVED VERSION v2\n\n**Based on User Feedback:**\n- NER Model: **90.2% F1** \u2705 Excellent (keeping same approach)\n- Sentiment Model: **58.0% F1** \u2192 **Target: 70%+** \ud83c\udfaf\n\n**\ud83d\ude80 SENTIMENT IMPROVEMENTS INTEGRATED:**\n1. \u2705 **Data Augmentation** - Expand 212 samples to ~640 using synonym replacement & contextual substitution (+5-10% F1)\n2. \u2705 **Focal Loss** - Better handle class imbalance, focus on hard examples (+3-5% F1)\n3. \u2705 **Optimized Hyperparameters** - Lower LR (1e-5), more epochs (10), cosine warmup (0.2) (+2-4% F1)\n4. \u2705 **Early Stopping** - Stop when val F1 plateaus (patience=3) (+1-2% F1)\n5. \u2705 **Better Contextualization** - Improved prompt: \"Tweet: {text} | Sentiment about {competitor}:\" (+1-3% F1)\n6. \u2705 **Detailed Analysis** - Confusion matrix, per-class F1, confidence analysis\n\n**Expected Total Improvement:** Sentiment F1: **68-75%** (from 58%)\n\n**All Previous Fixes Still Applied:**\n- \u2705 AdamW import fixed (torch.optim.AdamW)\n- \u2705 CSV UTF-8 auto-conversion\n- \u2705 Excel formatted output with color-coding\n- \u2705 Separate datasets for NER (3,183 rows) and Sentiment (265 rows)\n- \u2705 Single-label NER classification\n- \u2705 No more KeyError: 'SENTIMENT'\n\n**Pipeline:**\n1. Load and convert CSVs to UTF-8\n2. Prepare separate datasets for NER and Sentiment\n3. Train NER classifier (14-class) - **Keep excellent 90.2% F1**\n4. **[NEW]** Augment sentiment training data (212 \u2192 ~640 samples)\n5. **[NEW]** Train sentiment model with Focal Loss, early stopping, optimized hyperparameters\n6. **[NEW]** Detailed sentiment analysis with confusion matrix\n7. Generate predictions using hybrid NER + regex approach\n8. Export to formatted Excel"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries\n!pip install -q transformers datasets torch torchvision accelerate\n!pip install -q scikit-learn pandas numpy matplotlib seaborn\n!pip install -q sentencepiece protobuf\n!pip install -q openpyxl xlsxwriter  # For Excel export\n!pip install -q chardet  # For encoding detection\n!pip install -q nlpaug  # For data augmentation (IMPROVED)\n\n# Download NLTK data for augmentation\nimport nltk\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('averaged_perceptron_tagger', quiet=True)\n\nprint(\"\u2713 All libraries installed\")\nprint(\"\u2713 NLTK data downloaded for augmentation\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport gc\nimport warnings\nimport chardet\nimport random\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim import AdamW  # FIXED: Use torch.optim.AdamW instead of transformers\n\n# Transformers\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification,\n    get_linear_schedule_with_warmup,\n    get_cosine_schedule_with_warmup  # IMPROVED: Cosine scheduler\n)\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix,\n    accuracy_score, f1_score, precision_recall_fscore_support\n)\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Data Augmentation (IMPROVED)\nimport nlpaug.augmenter.word as naw\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\n# Set seeds\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nprint(\"\u2713 Libraries imported successfully!\")\nprint(f\"\u2713 PyTorch version: {torch.__version__}\")\nprint(f\"\u2713 Using AdamW from: torch.optim\")\nprint(f\"\u2713 Data augmentation ready (nlpaug)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and configure device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    gpu_memory_gb = gpu_props.total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_props.name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
    "    \n",
    "    # Adaptive batch size\n",
    "    if gpu_memory_gb >= 15:\n",
    "        BATCH_SIZE = 16\n",
    "        GRAD_ACCUM_STEPS = 2\n",
    "    else:\n",
    "        BATCH_SIZE = 8\n",
    "        GRAD_ACCUM_STEPS = 4\n",
    "else:\n",
    "    BATCH_SIZE = 4\n",
    "    GRAD_ACCUM_STEPS = 8\n",
    "\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "print(f\"\\nBatch size: {BATCH_SIZE}, Accumulation: {GRAD_ACCUM_STEPS}, Effective: {EFFECTIVE_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p '/content/drive/MyDrive/KFC_ML_Models'\n",
    "!mkdir -p '/content/results'\n",
    "\n",
    "MODEL_SAVE_DIR = '/content/drive/MyDrive/KFC_ML_Models'\n",
    "RESULTS_DIR = '/content/results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Competitor list (14 total)\nCOMPETITORS = [\n    'Burger King', 'Deliveroo', \"Domino's\", 'Five Guys', 'Greggs',\n    'Just Eat', 'KFC', \"McDonald's\", \"Nando's\", \"Papa John's\",\n    'Pizza Hut', 'Pret a Manger', 'Taco Bell', 'Uber Eats'\n]\n\nSENTIMENT_MAP = {0: 'negative', 1: 'neutral', 2: 'positive'}\n\n# Model configs\nNER_MODEL_NAME = 'bert-base-uncased'\nSENTIMENT_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n\n# Hyperparameters\nMAX_SEQ_LENGTH = 128\n\n# NER hyperparameters (KEEP - performing excellently at 90.2% F1)\nNER_LEARNING_RATE = 2e-5\nNER_EPOCHS = 5\nNER_WARMUP_RATIO = 0.1\n\n# IMPROVED Sentiment hyperparameters (targeting 70%+ F1 from 58%)\nSENTIMENT_LEARNING_RATE = 1e-5  # Lower LR for finer adjustments\nSENTIMENT_EPOCHS = 10  # More training time\nSENTIMENT_WARMUP_RATIO = 0.2  # More gradual warmup\nSENTIMENT_EARLY_STOP_PATIENCE = 3  # Early stopping\nAUGMENTATION_FACTOR = 3  # 212 samples \u2192 ~640 samples\n\n# Other\nWEIGHT_DECAY = 0.01\nFOCAL_LOSS_GAMMA = 2.0  # Focal loss parameter\n\nprint(f\"\u2713 Configuration loaded\")\nprint(f\"  Competitors: {len(COMPETITORS)}\")\nprint(f\"  NER Model: {NER_MODEL_NAME}\")\nprint(f\"  Sentiment Model: {SENTIMENT_MODEL_NAME}\")\nprint(f\"\\\\n\ud83c\udfaf SENTIMENT IMPROVEMENTS:\")\nprint(f\"  Learning Rate: {SENTIMENT_LEARNING_RATE} (was 2e-5)\")\nprint(f\"  Epochs: {SENTIMENT_EPOCHS} (was 5)\")\nprint(f\"  Warmup Ratio: {SENTIMENT_WARMUP_RATIO} (was 0.1)\")\nprint(f\"  Augmentation Factor: {AUGMENTATION_FACTOR}x\")\nprint(f\"  Early Stopping Patience: {SENTIMENT_EARLY_STOP_PATIENCE}\")\nprint(f\"  Focal Loss Gamma: {FOCAL_LOSS_GAMMA}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CSV Loading with UTF-8 Conversion (FIXED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    \"\"\"\n",
    "    Detect file encoding using chardet.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(100000))  # Read first 100KB\n",
    "    return result['encoding'], result['confidence']\n",
    "\n",
    "def convert_to_utf8(input_file, output_file=None):\n",
    "    \"\"\"\n",
    "    Convert CSV file to UTF-8 encoding.\n",
    "    If output_file is None, overwrites input_file.\n",
    "    \"\"\"\n",
    "    if output_file is None:\n",
    "        output_file = input_file\n",
    "    \n",
    "    # Detect encoding\n",
    "    encoding, confidence = detect_encoding(input_file)\n",
    "    print(f\"  Detected encoding: {encoding} (confidence: {confidence:.2%})\")\n",
    "    \n",
    "    if encoding and encoding.lower() != 'utf-8':\n",
    "        # Read with detected encoding\n",
    "        try:\n",
    "            with open(input_file, 'r', encoding=encoding, errors='replace') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Write as UTF-8\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            print(f\"  \u2713 Converted to UTF-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \u26a0 Conversion failed: {e}\")\n",
    "            print(f\"  Trying with 'utf-8' and error handling...\")\n",
    "    else:\n",
    "        print(f\"  \u2713 Already UTF-8\")\n",
    "\n",
    "def load_csv_safe(file_path):\n",
    "    \"\"\"\n",
    "    Load CSV with automatic UTF-8 conversion.\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading: {file_path}\")\n",
    "    \n",
    "    # Convert to UTF-8 first\n",
    "    convert_to_utf8(file_path)\n",
    "    \n",
    "    # Load with pandas\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)\n",
    "        print(f\"  \u2713 Loaded {len(df)} rows\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"  \u26a0 Error loading with UTF-8: {e}\")\n",
    "        print(f\"  Trying with encoding detection...\")\n",
    "        encoding, _ = detect_encoding(file_path)\n",
    "        df = pd.read_csv(file_path, encoding=encoding, low_memory=False, errors='replace')\n",
    "        print(f\"  \u2713 Loaded {len(df)} rows with {encoding}\")\n",
    "        return df\n",
    "\n",
    "print(\"\u2713 CSV loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload CSV files\n",
    "from google.colab import files\n",
    "print(\"Please upload your CSV files:\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets with UTF-8 conversion\n",
    "print(\"Loading and converting datasets to UTF-8...\")\n",
    "\n",
    "df_large = load_csv_safe('KFC_social_data.xlsx - Sheet1.csv')\n",
    "df_train_sample = load_csv_safe('KFC_training_sample.csv')\n",
    "df_test = load_csv_safe('KFC_test_sample.csv')\n",
    "df_test_pred = load_csv_safe('KFC_test_sample_for_prediction.csv')\n",
    "\n",
    "print(f\"\\n\u2713 All datasets loaded and converted to UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def clean_sentiment(value):\n    \"\"\"Extract numeric sentiment (0, 1, 2)\"\"\"\n    if pd.isna(value):\n        return None\n    \n    if isinstance(value, (int, float)):\n        if value in [0, 1, 2]:\n            return int(value)\n        return None\n    \n    value_str = str(value).strip().lower()\n    if value_str in ['0', 'negative']: return 0\n    elif value_str in ['1', 'neutral']: return 1\n    elif value_str in ['2', 'positive']: return 2\n    \n    match = re.match(r'^(\\d)', value_str)\n    if match:\n        digit = int(match.group(1))\n        if digit in [0, 1, 2]:\n            return digit\n    \n    return None\n\ndef normalize_competitor_name(comp_str):\n    \"\"\"Normalize competitor names\"\"\"\n    if pd.isna(comp_str) or not comp_str:\n        return None\n    \n    comp_str = str(comp_str).strip()\n    \n    if comp_str in COMPETITORS:\n        return comp_str\n    \n    for comp in COMPETITORS:\n        if comp.lower() == comp_str.lower():\n            return comp\n    \n    return None\n\ndef prepare_dataset(df, name=\"dataset\"):\n    \"\"\"Clean and prepare dataset\"\"\"\n    print(f\"\\nPreparing {name}...\")\n    print(f\"  Initial rows: {len(df)}\")\n    \n    # Select columns\n    essential_cols = ['Competitor', 'Tweet', 'SENTIMENT']\n    if 'Tweet' not in df.columns:\n        if 'Full Text' in df.columns:\n            df['Tweet'] = df['Full Text']\n        elif 'Snippet' in df.columns:\n            df['Tweet'] = df['Snippet']\n    \n    available_cols = [col for col in essential_cols if col in df.columns]\n    metadata_cols = [col for col in ['Impact', 'Impressions', 'Reach (new)', 'Date', 'Url'] if col in df.columns]\n    \n    df_clean = df[available_cols + metadata_cols].copy()\n    \n    # Clean sentiment\n    if 'SENTIMENT' in df_clean.columns:\n        df_clean['SENTIMENT'] = df_clean['SENTIMENT'].apply(clean_sentiment)\n        before = len(df_clean)\n        df_clean = df_clean.dropna(subset=['SENTIMENT'])\n        print(f\"  Dropped {before - len(df_clean)} rows with invalid sentiment\")\n        df_clean['SENTIMENT'] = df_clean['SENTIMENT'].astype(int)\n    \n    # Clean competitor names\n    df_clean['Competitor'] = df_clean['Competitor'].apply(normalize_competitor_name)\n    before = len(df_clean)\n    df_clean = df_clean.dropna(subset=['Competitor', 'Tweet'])\n    print(f\"  Dropped {before - len(df_clean)} rows with invalid competitor/tweet\")\n    \n    # Clean tweet text\n    df_clean['Tweet'] = df_clean['Tweet'].astype(str).str.strip()\n    df_clean = df_clean[df_clean['Tweet'].str.len() > 0]\n    \n    df_clean = df_clean.reset_index(drop=True)\n    \n    print(f\"  Final rows: {len(df_clean)}\")\n    print(f\"  Unique competitors: {df_clean['Competitor'].nunique()}\")\n    \n    if 'SENTIMENT' in df_clean.columns:\n        sent_dist = df_clean['SENTIMENT'].value_counts().sort_index()\n        print(f\"  Sentiment distribution: {dict(sent_dist)}\")\n    \n    return df_clean\n\n# CRITICAL FIX: Prepare datasets separately for NER and Sentiment\nprint(\"=\"*70)\nprint(\"PREPARING DATASETS FOR DIFFERENT TASKS\")\nprint(\"=\"*70)\nprint(\"NER model needs: 'Competitor' labels\")\nprint(\"Sentiment model needs: 'SENTIMENT' labels\")\nprint(\"=\"*70)\n\n# NER Dataset (use large dataset - has Competitor labels)\ndf_large_clean = prepare_dataset(df_large, \"Large dataset for NER\")\n\n# Sentiment Dataset (use training sample - has SENTIMENT labels)\ndf_train_sample_clean = prepare_dataset(df_train_sample, \"Training sample for Sentiment\")\n\n# Test datasets\ndf_test_clean = prepare_dataset(df_test, \"Test dataset\")\n\n# For prediction data (no sentiment)\nif 'SENTIMENT' not in df_test_pred.columns:\n    df_test_pred['SENTIMENT'] = 1  # Dummy value\ndf_test_pred_clean = prepare_dataset(df_test_pred, \"Test prediction dataset\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DATASET SUMMARY:\")\nprint(f\"  NER training data: {len(df_large_clean)} rows (from large CSV)\")\nprint(f\"  Sentiment training data: {len(df_train_sample_clean)} rows (from training sample)\")\nprint(f\"  Test data: {len(df_test_clean)} rows\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Competitor Extraction (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_competitors(tweet_text):\n",
    "    \"\"\"\n",
    "    Extract ALL competitors mentioned using regex.\n",
    "    \"\"\"\n",
    "    found_competitors = set()\n",
    "    tweet_lower = tweet_text.lower()\n",
    "    \n",
    "    patterns = {\n",
    "        'Burger King': [r'\\bburger\\s*king\\b', r'\\bbk\\b'],\n",
    "        'Deliveroo': [r'\\bdeliveroo\\b'],\n",
    "        \"Domino's\": [r'\\bdomino(?:s|\\'s)?\\b'],\n",
    "        'Five Guys': [r'\\bfive\\s*guys\\b'],\n",
    "        'Greggs': [r'\\bgreggs?\\b'],\n",
    "        'Just Eat': [r'\\bjust\\s*eat\\b'],\n",
    "        'KFC': [r'\\bkfc\\b', r'\\bkentucky\\s*fried\\s*chicken\\b', r'@kfc'],\n",
    "        \"McDonald's\": [r'\\bmcdonald(?:s|\\'s)?\\b', r'\\bmaccies\\b', r'\\bmaccas\\b', r'\\bmcdonalds\\b', r'@mcdonald'],\n",
    "        \"Nando's\": [r'\\bnando(?:s|\\'s)\\b', r'@nando'],\n",
    "        \"Papa John's\": [r'\\bpapa\\s*john(?:s|\\'s)?\\b', r'@papajohn'],\n",
    "        'Pizza Hut': [r'\\bpizza\\s*hut\\b', r'@pizzahut'],\n",
    "        'Pret a Manger': [r'\\bpret(?:\\s*a\\s*manger)?\\b', r'@pret'],\n",
    "        'Taco Bell': [r'\\btaco\\s*bell\\b', r'@tacobell'],\n",
    "        'Uber Eats': [r'\\buber\\s*eats\\b', r'@ubereats']\n",
    "    }\n",
    "    \n",
    "    for competitor, pattern_list in patterns.items():\n",
    "        for pattern in pattern_list:\n",
    "            if re.search(pattern, tweet_lower):\n",
    "                found_competitors.add(competitor)\n",
    "                break\n",
    "    \n",
    "    return list(found_competitors)\n",
    "\n",
    "# Test\n",
    "test_tweet = \"I love KFC's chicken but McDonald's has better fries!\"\n",
    "print(f\"Test tweet: {test_tweet}\")\n",
    "print(f\"Found competitors: {extract_all_competitors(test_tweet)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TRAIN/VAL SPLIT - SEPARATE FOR NER AND SENTIMENT (FIXED)\n# ============================================================\n\n# NER: Split large dataset (for competitor identification)\nprint(\"\\n\" + \"=\"*70)\nprint(\"Creating NER train/val split...\")\nprint(\"=\"*70)\nner_train_df, ner_val_df = train_test_split(\n    df_large_clean,\n    test_size=0.2,\n    random_state=SEED,\n    stratify=df_large_clean['Competitor']\n)\n\nprint(f\"NER Dataset Split:\")\nprint(f\"  Training: {len(ner_train_df)} samples\")\nprint(f\"  Validation: {len(ner_val_df)} samples\")\nprint(f\"  Competitors: {ner_train_df['Competitor'].nunique()}\")\nprint(f\"\\nTop 5 competitors in NER training set:\")\nprint(ner_train_df['Competitor'].value_counts().head())\n\n# Sentiment: Split training sample (for sentiment classification)\nprint(\"\\n\" + \"=\"*70)\nprint(\"Creating Sentiment train/val split...\")\nprint(\"=\"*70)\nsentiment_train_df, sentiment_val_df = train_test_split(\n    df_train_sample_clean,\n    test_size=0.2,\n    random_state=SEED,\n    stratify=df_train_sample_clean['SENTIMENT']\n)\n\nprint(f\"Sentiment Dataset Split:\")\nprint(f\"  Training: {len(sentiment_train_df)} samples\")\nprint(f\"  Validation: {len(sentiment_val_df)} samples\")\nprint(f\"  Sentiment distribution in training:\")\nsentiment_dist = sentiment_train_df['SENTIMENT'].value_counts().sort_index()\nfor sent, count in sentiment_dist.items():\n    print(f\"    {SENTIMENT_MAP[sent]:8s}: {count} ({count/len(sentiment_train_df)*100:.1f}%)\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(f\"Test data: {len(df_test_clean)} samples\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b. Sentiment Data Augmentation (IMPROVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA AUGMENTATION FOR SENTIMENT TRAINING (IMPROVED)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AUGMENTING SENTIMENT TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original training samples: {len(sentiment_train_df)}\")\n",
    "print(f\"Target: {len(sentiment_train_df) * AUGMENTATION_FACTOR} samples ({AUGMENTATION_FACTOR}x)\")\n",
    "\n",
    "# Create augmenters\n",
    "try:\n",
    "    aug_synonym = naw.SynonymAug(aug_src='wordnet', aug_p=0.3)\n",
    "    print(\"\u2713 Synonym augmenter loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0 Synonym augmenter failed: {e}\")\n",
    "    aug_synonym = None\n",
    "\n",
    "try:\n",
    "    aug_contextual = naw.ContextualWordEmbsAug(\n",
    "        model_path='bert-base-uncased',\n",
    "        action=\"substitute\",\n",
    "        aug_p=0.3,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    print(\"\u2713 Contextual augmenter loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0 Contextual augmenter failed: {e}\")\n",
    "    aug_contextual = None\n",
    "\n",
    "# Augment data\n",
    "augmented_rows = []\n",
    "\n",
    "for idx, row in tqdm(sentiment_train_df.iterrows(), total=len(sentiment_train_df), desc=\"Augmenting\"):\n",
    "    # Add original\n",
    "    augmented_rows.append(row.to_dict())\n",
    "    \n",
    "    tweet = row['Tweet']\n",
    "    \n",
    "    # Try synonym replacement\n",
    "    if aug_synonym is not None and len(augmented_rows) < (idx + 1) * AUGMENTATION_FACTOR:\n",
    "        try:\n",
    "            aug_text = aug_synonym.augment(tweet)\n",
    "            if isinstance(aug_text, list):\n",
    "                aug_text = aug_text[0]\n",
    "            if aug_text != tweet:\n",
    "                aug_row = row.to_dict()\n",
    "                aug_row['Tweet'] = aug_text\n",
    "                augmented_rows.append(aug_row)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try contextual substitution\n",
    "    if aug_contextual is not None and len(augmented_rows) < (idx + 1) * AUGMENTATION_FACTOR:\n",
    "        try:\n",
    "            aug_text = aug_contextual.augment(tweet)\n",
    "            if isinstance(aug_text, list):\n",
    "                aug_text = aug_text[0]\n",
    "            if aug_text != tweet:\n",
    "                aug_row = row.to_dict()\n",
    "                aug_row['Tweet'] = aug_text\n",
    "                augmented_rows.append(aug_row)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "sentiment_train_df_augmented = pd.DataFrame(augmented_rows)\n",
    "\n",
    "print(f\"\\n\u2713 Augmentation complete!\")\n",
    "print(f\"  Original: {len(sentiment_train_df)} samples\")\n",
    "print(f\"  Augmented: {len(sentiment_train_df_augmented)} samples\")\n",
    "print(f\"  Increase: {len(sentiment_train_df_augmented) / len(sentiment_train_df):.1f}x\")\n",
    "\n",
    "print(f\"\\nAugmented Sentiment Distribution:\")\n",
    "for sent, count in sentiment_train_df_augmented['SENTIMENT'].value_counts().sort_index().items():\n",
    "    print(f\"  {SENTIMENT_MAP[sent]:8s}: {count} ({count/len(sentiment_train_df_augmented)*100:.1f}%)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6b. Sentiment Data Augmentation (IMPROVED)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 6b. Improvement Classes (Focal Loss & Early Stopping)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. NER Model - Single-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetitorDataset(Dataset):\n",
    "    \"\"\"Dataset for competitor classification (0-13)\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.competitor_to_idx = {comp: idx for idx, comp in enumerate(COMPETITORS)}\n",
    "        \n",
    "        # Validation\n",
    "        unique_comps = self.data['Competitor'].unique()\n",
    "        print(f\"\\nDataset has {len(unique_comps)} unique competitors:\")\n",
    "        for comp in unique_comps:\n",
    "            if comp in self.competitor_to_idx:\n",
    "                count = (self.data['Competitor'] == comp).sum()\n",
    "                print(f\"  \u2713 {comp}: {count} samples (label {self.competitor_to_idx[comp]})\")\n",
    "            else:\n",
    "                print(f\"  \u2717 {comp}: NOT IN COMPETITOR LIST!\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        tweet = row['Tweet']\n",
    "        competitor = row['Competitor']\n",
    "        label = self.competitor_to_idx[competitor]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"\u2713 CompetitorDataset defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load NER tokenizer\nprint(f\"Loading NER tokenizer: {NER_MODEL_NAME}\")\nner_tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_NAME)\n\n# Create datasets - FIXED: Use ner_train_df and ner_val_df\nprint(\"\\nCreating NER datasets...\")\nner_train_dataset = CompetitorDataset(ner_train_df, ner_tokenizer, MAX_SEQ_LENGTH)\nner_val_dataset = CompetitorDataset(ner_val_df, ner_tokenizer, MAX_SEQ_LENGTH)\nner_test_dataset = CompetitorDataset(df_test_clean, ner_tokenizer, MAX_SEQ_LENGTH)\n\n# DataLoaders\nner_train_loader = DataLoader(ner_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nner_val_loader = DataLoader(ner_val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\nner_test_loader = DataLoader(ner_test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\n\nprint(f\"\\n\u2713 NER DataLoaders created\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Class weights for imbalanced data - FIXED: Use ner_train_df\ntrain_labels = [ner_train_dataset.competitor_to_idx[comp] for comp in ner_train_df['Competitor']]\nner_class_weights = compute_class_weight(\n    'balanced',\n    classes=np.arange(len(COMPETITORS)),\n    y=train_labels\n)\nner_class_weights = torch.tensor(ner_class_weights, dtype=torch.float).to(device)\n\nprint(\"NER Class Weights:\")\nfor i, comp in enumerate(COMPETITORS):\n    print(f\"  {comp:20s}: {ner_class_weights[i]:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6c. Focal Loss & Early Stopping Classes (IMPROVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FOCAL LOSS FOR HANDLING CLASS IMBALANCE (IMPROVED)\n",
    "# ============================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss: FL(p_t) = -\u03b1_t * (1 - p_t)^\u03b3 * log(p_t)\n",
    "    \n",
    "    Focuses training on hard-to-classify examples.\n",
    "    Down-weights easy examples to prevent them from dominating training.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # Class weights\n",
    "        self.gamma = gamma  # Focusing parameter\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
    "        p_t = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - p_t) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# ============================================================\n",
    "# EARLY STOPPING TO PREVENT OVERFITTING (IMPROVED)\n",
    "# ============================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Stop training when validation F1 stops improving\"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.001, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            return False\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            improved = val_score > self.best_score + self.min_delta\n",
    "        else:\n",
    "            improved = val_score < self.best_score - self.min_delta\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "print(\"\u2713 FocalLoss class defined\")\n",
    "print(\"\u2713 EarlyStopping class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, class_weights, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Training function with torch.optim.AdamW (FIXED)\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # FIXED: Use torch.optim.AdamW\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs // GRAD_ACCUM_STEPS\n",
    "    warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_f1': []}\n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    print(f\"  Epochs: {epochs}, Steps: {total_steps}, Warmup: {warmup_steps}\")\n",
    "    print(f\"  Using optimizer: torch.optim.AdamW\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for step, batch in enumerate(train_pbar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs.logits, labels) / GRAD_ACCUM_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item() * GRAD_ACCUM_STEPS:.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            print(f\"  \u2713 New best F1! Saving model...\")\n",
    "            torch.save(model.state_dict(), f'{MODEL_SAVE_DIR}/{model_name}_best.pt')\n",
    "        print()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\n\u2713 Training complete! Best val F1: {best_val_f1:.4f}\")\n",
    "    return model, history\n",
    "\n",
    "print(\"\u2713 Training function defined (using torch.optim.AdamW)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentiment_model_improved(model, train_loader, val_loader, epochs, learning_rate,\n",
    "                                   class_weights, early_stopping_patience=3, model_name=\"sentiment_model\"):\n",
    "    \"\"\"\n",
    "    IMPROVED training function for sentiment model with:\n",
    "    - Focal Loss\n",
    "    - Early Stopping\n",
    "    - Cosine scheduler\n",
    "    - Better progress tracking\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs // GRAD_ACCUM_STEPS\n",
    "    warmup_steps = int(total_steps * SENTIMENT_WARMUP_RATIO)\n",
    "    \n",
    "    # IMPROVED: Use cosine scheduler for sentiment\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    # IMPROVED: Use Focal Loss instead of CrossEntropyLoss\n",
    "    criterion = FocalLoss(alpha=class_weights, gamma=FOCAL_LOSS_GAMMA)\n",
    "    \n",
    "    # IMPROVED: Add early stopping\n",
    "    early_stopping = EarlyStopping(patience=early_stopping_patience, mode='max')\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_f1': []}\n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    print(f\"\\nTraining {model_name} with IMPROVEMENTS...\")\n",
    "    print(f\"  Epochs: {epochs}, Steps: {total_steps}, Warmup: {warmup_steps}\")\n",
    "    print(f\"  Learning Rate: {learning_rate}\")\n",
    "    print(f\"  Using: Focal Loss (gamma={FOCAL_LOSS_GAMMA}), Cosine Scheduler, Early Stopping (patience={early_stopping_patience})\")\n",
    "    print(f\"  Training samples: {len(train_loader.dataset)}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for step, batch in enumerate(train_pbar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs.logits, labels) / GRAD_ACCUM_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item() * GRAD_ACCUM_STEPS:.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  Val F1 (macro): {val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            print(f\"  \u2713 New best F1! Saving model...\")\n",
    "            torch.save(model.state_dict(), f'{MODEL_SAVE_DIR}/{model_name}_best.pt')\n",
    "        \n",
    "        # IMPROVED: Check early stopping\n",
    "        if early_stopping(val_f1):\n",
    "            print(f\"  \u26a0 Early stopping triggered (no improvement for {early_stopping_patience} epochs)\")\n",
    "            break\n",
    "        \n",
    "        print()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\n\u2713 Training complete! Best val F1: {best_val_f1:.4f}\")\n",
    "    print(f\"  Stopped at epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"\u2713 Improved sentiment training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NER model\n",
    "print(f\"Initializing NER model: {NER_MODEL_NAME}\")\n",
    "ner_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    NER_MODEL_NAME,\n",
    "    num_labels=len(COMPETITORS)\n",
    ")\n",
    "\n",
    "ner_model, ner_history = train_model(\n",
    "    ner_model,\n",
    "    ner_train_loader,\n",
    "    ner_val_loader,\n",
    "    epochs=NER_EPOCHS,\n",
    "    learning_rate=NER_LEARNING_RATE,\n",
    "    class_weights=ner_class_weights,\n",
    "    model_name=\"ner_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot NER history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(ner_history['train_loss'], label='Train', marker='o')\n",
    "axes[0].plot(ner_history['val_loss'], label='Val', marker='s')\n",
    "axes[0].set_title('NER - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(ner_history['val_accuracy'], marker='o', color='blue')\n",
    "axes[1].set_title('NER - Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(ner_history['val_f1'], marker='o', color='green')\n",
    "axes[2].set_title('NER - Validation F1', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/ner_training.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\u2713 Best NER F1: {max(ner_history['val_f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"IMPROVED: Competitor-aware sentiment dataset with better contextualization\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        tweet = row['Tweet']\n",
    "        competitor = row['Competitor']\n",
    "        sentiment = row['SENTIMENT']\n",
    "        \n",
    "        # IMPROVED: Better contextualization\n",
    "        text = f\"Tweet: {tweet} | Sentiment about {competitor}:\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(sentiment, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"\u2713 IMPROVED SentimentDataset defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentiment tokenizer\n",
    "print(f\"Loading Sentiment tokenizer: {SENTIMENT_MODEL_NAME}\")\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(SENTIMENT_MODEL_NAME)\n",
    "\n",
    "# Create datasets - IMPROVED: Use AUGMENTED training data\n",
    "print(\"\\nCreating IMPROVED Sentiment datasets...\")\n",
    "sentiment_train_dataset = SentimentDataset(sentiment_train_df_augmented, sentiment_tokenizer, MAX_SEQ_LENGTH)  # AUGMENTED!\n",
    "sentiment_val_dataset = SentimentDataset(sentiment_val_df, sentiment_tokenizer, MAX_SEQ_LENGTH)  # Keep validation original\n",
    "sentiment_test_dataset = SentimentDataset(df_test_clean, sentiment_tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "print(f\"  Train (AUGMENTED): {len(sentiment_train_dataset)} samples\")\n",
    "print(f\"  Val: {len(sentiment_val_dataset)} samples\")\n",
    "print(f\"  Test: {len(sentiment_test_dataset)} samples\")\n",
    "\n",
    "# DataLoaders\n",
    "sentiment_train_loader = DataLoader(sentiment_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "sentiment_val_loader = DataLoader(sentiment_val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\n",
    "sentiment_test_loader = DataLoader(sentiment_test_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\n\u2713 IMPROVED Sentiment DataLoaders created with augmented training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment class weights - IMPROVED: Use augmented training data\n",
    "sentiment_class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.arange(3),\n",
    "    y=sentiment_train_df_augmented['SENTIMENT'].values  # AUGMENTED!\n",
    ")\n",
    "sentiment_class_weights = torch.tensor(sentiment_class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(\"Sentiment Class Weights (from augmented data):\")\n",
    "for i, label in SENTIMENT_MAP.items():\n",
    "    print(f\"  {label:8s}: {sentiment_class_weights[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Sentiment model with IMPROVEMENTS\n",
    "print(f\"Initializing Sentiment model: {SENTIMENT_MODEL_NAME}\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    SENTIMENT_MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# IMPROVED: Use new training function with all improvements\n",
    "sentiment_model, sentiment_history = train_sentiment_model_improved(\n",
    "    sentiment_model,\n",
    "    sentiment_train_loader,\n",
    "    sentiment_val_loader,\n",
    "    epochs=SENTIMENT_EPOCHS,\n",
    "    learning_rate=SENTIMENT_LEARNING_RATE,\n",
    "    class_weights=sentiment_class_weights,\n",
    "    early_stopping_patience=SENTIMENT_EARLY_STOP_PATIENCE,\n",
    "    model_name=\"sentiment_model_improved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sentiment history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(sentiment_history['train_loss'], label='Train', marker='o')\n",
    "axes[0].plot(sentiment_history['val_loss'], label='Val', marker='s')\n",
    "axes[0].set_title('Sentiment - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(sentiment_history['val_accuracy'], marker='o', color='blue')\n",
    "axes[1].set_title('Sentiment - Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(sentiment_history['val_f1'], marker='o', color='green')\n",
    "axes[2].set_title('Sentiment - Validation F1', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/sentiment_training.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\u2713 Best Sentiment F1: {max(sentiment_history['val_f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8b. Detailed Sentiment Analysis (IMPROVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DETAILED SENTIMENT PERFORMANCE ANALYSIS (IMPROVED)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SENTIMENT MODEL DETAILED ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sentiment_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(sentiment_val_loader, desc=\"Analyzing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = sentiment_model(input_ids, attention_mask)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=['negative', 'neutral', 'positive'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['negative', 'neutral', 'positive'],\n",
    "    yticklabels=['negative', 'neutral', 'positive']\n",
    ")\n",
    "plt.title('Sentiment Confusion Matrix - IMPROVED MODEL', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/sentiment_confusion_matrix_improved.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, average=None\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(f\"{'Class':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Support':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for i, label in enumerate(['negative', 'neutral', 'positive']):\n",
    "    print(f\"{label:<10} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1[i]:<10.4f} {support[i]:<10}\")\n",
    "\n",
    "# Overall metrics\n",
    "overall_f1 = f1.mean()\n",
    "print(f\"\\nOverall F1 (Macro Avg): {overall_f1:.4f}\")\n",
    "print(f\"Improvement from baseline: {overall_f1 - 0.58:.4f} ({(overall_f1 - 0.58)/0.58*100:+.1f}% change)\")\n",
    "\n",
    "# Confidence analysis\n",
    "avg_confidence = np.max(all_probs, axis=1).mean()\n",
    "print(f\"\\nAverage Prediction Confidence: {avg_confidence:.3f}\")\n",
    "\n",
    "# Misclassifications\n",
    "misclassified = all_preds != all_labels\n",
    "num_misclass = misclassified.sum()\n",
    "print(f\"\\nMisclassifications: {num_misclass} / {len(all_labels)} ({num_misclass/len(all_labels)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 Detailed analysis complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integrated Pipeline & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet_text, ner_model, sentiment_model, ner_tokenizer, sentiment_tokenizer):\n",
    "    \"\"\"\n",
    "    Full pipeline: NER + Regex \u2192 Sentiment per competitor\n",
    "    \"\"\"\n",
    "    ner_model.eval()\n",
    "    sentiment_model.eval()\n",
    "    \n",
    "    # NER prediction\n",
    "    encoding = ner_tokenizer(\n",
    "        tweet_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            outputs = ner_model(encoding['input_ids'].to(device), encoding['attention_mask'].to(device))\n",
    "    \n",
    "    predicted_idx = torch.argmax(outputs.logits, dim=1).item()\n",
    "    primary_competitor = COMPETITORS[predicted_idx]\n",
    "    \n",
    "    # Regex extraction\n",
    "    regex_competitors = extract_all_competitors(tweet_text)\n",
    "    \n",
    "    # Combine\n",
    "    all_competitors = set([primary_competitor] + regex_competitors)\n",
    "    \n",
    "    # Sentiment for each\n",
    "    results = []\n",
    "    for competitor in all_competitors:\n",
    "        text = f\"{tweet_text} This tweet is about {competitor}.\"\n",
    "        \n",
    "        sentiment_encoding = sentiment_tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                sentiment_outputs = sentiment_model(\n",
    "                    sentiment_encoding['input_ids'].to(device),\n",
    "                    sentiment_encoding['attention_mask'].to(device)\n",
    "                )\n",
    "        \n",
    "        predicted_sentiment = torch.argmax(sentiment_outputs.logits, dim=1).item()\n",
    "        results.append((competitor, predicted_sentiment))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "test_tweet = \"KFC's chicken is amazing but McDonald's has terrible service\"\n",
    "print(f\"Test tweet: {test_tweet}\")\n",
    "predictions = predict_tweet(test_tweet, ner_model, sentiment_model, ner_tokenizer, sentiment_tokenizer)\n",
    "for comp, sent in predictions:\n",
    "    print(f\"  {comp}: {SENTIMENT_MAP[sent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(df, ner_model, sentiment_model, ner_tokenizer, sentiment_tokenizer):\n",
    "    \"\"\"\n",
    "    Process entire dataset with pipeline.\n",
    "    Returns DataFrame with one row per detected competitor.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing tweets\"):\n",
    "        tweet = row['Tweet']\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = predict_tweet(tweet, ner_model, sentiment_model, ner_tokenizer, sentiment_tokenizer)\n",
    "        \n",
    "        # Create row for each detected competitor\n",
    "        for competitor, sentiment in predictions:\n",
    "            result_row = {\n",
    "                'Competitor': competitor,\n",
    "                'Tweet': tweet,\n",
    "                'Predicted_Sentiment': sentiment,\n",
    "                'Sentiment_Label': SENTIMENT_MAP[sentiment]\n",
    "            }\n",
    "            \n",
    "            # Add metadata\n",
    "            for col in ['Impact', 'Impressions', 'Reach (new)', 'Date', 'Url']:\n",
    "                if col in row.index and pd.notna(row[col]):\n",
    "                    result_row[col] = row[col]\n",
    "            \n",
    "            results.append(result_row)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"\u2713 Batch processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test data\n",
    "print(\"Generating predictions for test data...\\n\")\n",
    "\n",
    "# Load best models\n",
    "ner_model.load_state_dict(torch.load(f'{MODEL_SAVE_DIR}/ner_model_best.pt'))\n",
    "sentiment_model.load_state_dict(torch.load(f'{MODEL_SAVE_DIR}/sentiment_model_best.pt'))\n",
    "\n",
    "# Process\n",
    "predictions_df = process_dataset(\n",
    "    df_test_pred_clean,\n",
    "    ner_model,\n",
    "    sentiment_model,\n",
    "    ner_tokenizer,\n",
    "    sentiment_tokenizer\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Generated {len(predictions_df)} predictions from {len(df_test_pred_clean)} tweets\")\n",
    "print(f\"  Average competitors per tweet: {len(predictions_df) / len(df_test_pred_clean):.2f}\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Excel Export with Formatting (FIXED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_excel(df, filename, include_summary=True):\n",
    "    \"\"\"\n",
    "    Export predictions to formatted Excel file.\n",
    "    \"\"\"\n",
    "    filepath = f'{RESULTS_DIR}/{filename}'\n",
    "    \n",
    "    # Create Excel writer\n",
    "    with pd.ExcelWriter(filepath, engine='xlsxwriter') as writer:\n",
    "        # Write main data\n",
    "        df.to_excel(writer, sheet_name='Predictions', index=False)\n",
    "        \n",
    "        # Get workbook and worksheet\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['Predictions']\n",
    "        \n",
    "        # Define formats\n",
    "        header_format = workbook.add_format({\n",
    "            'bold': True,\n",
    "            'text_wrap': True,\n",
    "            'valign': 'top',\n",
    "            'fg_color': '#D7E4BD',\n",
    "            'border': 1\n",
    "        })\n",
    "        \n",
    "        # Sentiment color formats\n",
    "        negative_format = workbook.add_format({'bg_color': '#FFC7CE', 'font_color': '#9C0006'})\n",
    "        neutral_format = workbook.add_format({'bg_color': '#FFEB9C', 'font_color': '#9C6500'})\n",
    "        positive_format = workbook.add_format({'bg_color': '#C6EFCE', 'font_color': '#006100'})\n",
    "        \n",
    "        # Apply header format\n",
    "        for col_num, value in enumerate(df.columns.values):\n",
    "            worksheet.write(0, col_num, value, header_format)\n",
    "        \n",
    "        # Set column widths\n",
    "        worksheet.set_column('A:A', 15)  # Competitor\n",
    "        worksheet.set_column('B:B', 60)  # Tweet\n",
    "        worksheet.set_column('C:C', 12)  # Predicted_Sentiment\n",
    "        worksheet.set_column('D:D', 15)  # Sentiment_Label\n",
    "        worksheet.set_column('E:Z', 12)  # Other columns\n",
    "        \n",
    "        # Apply conditional formatting for sentiment\n",
    "        if 'Sentiment_Label' in df.columns:\n",
    "            sentiment_col = df.columns.get_loc('Sentiment_Label')\n",
    "            \n",
    "            # Negative\n",
    "            worksheet.conditional_format(1, sentiment_col, len(df), sentiment_col, {\n",
    "                'type': 'text',\n",
    "                'criteria': 'containing',\n",
    "                'value': 'negative',\n",
    "                'format': negative_format\n",
    "            })\n",
    "            \n",
    "            # Neutral\n",
    "            worksheet.conditional_format(1, sentiment_col, len(df), sentiment_col, {\n",
    "                'type': 'text',\n",
    "                'criteria': 'containing',\n",
    "                'value': 'neutral',\n",
    "                'format': neutral_format\n",
    "            })\n",
    "            \n",
    "            # Positive\n",
    "            worksheet.conditional_format(1, sentiment_col, len(df), sentiment_col, {\n",
    "                'type': 'text',\n",
    "                'criteria': 'containing',\n",
    "                'value': 'positive',\n",
    "                'format': positive_format\n",
    "            })\n",
    "        \n",
    "        # Add summary sheet if requested\n",
    "        if include_summary:\n",
    "            summary_data = []\n",
    "            \n",
    "            # Overall stats\n",
    "            summary_data.append(['Metric', 'Value'])\n",
    "            summary_data.append(['Total Predictions', len(df)])\n",
    "            summary_data.append(['Unique Tweets', df['Tweet'].nunique()])\n",
    "            summary_data.append(['Unique Competitors', df['Competitor'].nunique()])\n",
    "            summary_data.append([''])\n",
    "            \n",
    "            # Sentiment distribution\n",
    "            summary_data.append(['Sentiment', 'Count', 'Percentage'])\n",
    "            sent_dist = df['Sentiment_Label'].value_counts()\n",
    "            for sent, count in sent_dist.items():\n",
    "                pct = count / len(df) * 100\n",
    "                summary_data.append([sent, count, f'{pct:.1f}%'])\n",
    "            \n",
    "            summary_data.append([''])\n",
    "            \n",
    "            # Per-competitor stats\n",
    "            summary_data.append(['Competitor', 'Mentions', 'Positive', 'Neutral', 'Negative'])\n",
    "            for comp in sorted(df['Competitor'].unique()):\n",
    "                comp_df = df[df['Competitor'] == comp]\n",
    "                pos = len(comp_df[comp_df['Sentiment_Label'] == 'positive'])\n",
    "                neu = len(comp_df[comp_df['Sentiment_Label'] == 'neutral'])\n",
    "                neg = len(comp_df[comp_df['Sentiment_Label'] == 'negative'])\n",
    "                summary_data.append([comp, len(comp_df), pos, neu, neg])\n",
    "            \n",
    "            # Write summary\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_excel(writer, sheet_name='Summary', index=False, header=False)\n",
    "            \n",
    "            # Format summary sheet\n",
    "            summary_ws = writer.sheets['Summary']\n",
    "            summary_ws.set_column('A:A', 20)\n",
    "            summary_ws.set_column('B:E', 12)\n",
    "    \n",
    "    print(f\"\\n\u2713 Excel file saved: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "print(\"\u2713 Excel export function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Excel\n",
    "excel_path = export_to_excel(\n",
    "    predictions_df,\n",
    "    'KFC_Predictions_Complete.xlsx',\n",
    "    include_summary=True\n",
    ")\n",
    "\n",
    "print(\"\\n\u2713 Predictions exported to formatted Excel file!\")\n",
    "print(f\"  File: {excel_path}\")\n",
    "print(f\"  Sheets: 'Predictions' (main data), 'Summary' (statistics)\")\n",
    "print(f\"  Format: Color-coded sentiment, auto-width columns, summary stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save to Google Drive\n",
    "drive_path = f'{MODEL_SAVE_DIR}/KFC_Predictions_Complete.xlsx'\n",
    "predictions_df.to_excel(drive_path, index=False, engine='openpyxl')\n",
    "print(f\"\\n\u2713 Also saved to Google Drive: {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING & PREDICTION COMPLETE - IMPROVED VERSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\u2705 All Fixes Applied:\")\n",
    "print(\"  1. AdamW import (torch.optim.AdamW)\")\n",
    "print(\"  2. CSV encoding (auto-converted to UTF-8)\")\n",
    "print(\"  3. Excel output (formatted .xlsx with color-coding)\")\n",
    "print(\"  4. NER model (single-label classification)\")\n",
    "print(\"  5. Separate datasets (NER: large, Sentiment: training sample)\")\n",
    "\n",
    "print(\"\\n\ud83d\ude80 Sentiment Improvements Applied:\")\n",
    "print(f\"  1. Data Augmentation: {len(sentiment_train_df)} \u2192 {len(sentiment_train_df_augmented)} samples\")\n",
    "print(f\"  2. Focal Loss (gamma={FOCAL_LOSS_GAMMA})\")\n",
    "print(f\"  3. Optimized Hyperparameters (LR={SENTIMENT_LEARNING_RATE}, Epochs={SENTIMENT_EPOCHS})\")\n",
    "print(f\"  4. Early Stopping (patience={SENTIMENT_EARLY_STOP_PATIENCE})\")\n",
    "print(\"  5. Better Contextualization\")\n",
    "print(\"  6. Detailed Analysis with Confusion Matrix\")\n",
    "\n",
    "print(\"\\n\ud83d\udcca Results:\")\n",
    "print(f\"  NER Best F1: {max(ner_history['val_f1']):.4f}\")\n",
    "print(f\"  Sentiment Best F1: {max(sentiment_history['val_f1']):.4f}\")\n",
    "print(f\"  Sentiment Improvement: {(max(sentiment_history['val_f1']) - 0.58) / 0.58 * 100:+.1f}% from baseline (0.58)\")\n",
    "print(f\"  Total predictions: {len(predictions_df)}\")\n",
    "print(f\"  From {len(df_test_pred_clean)} tweets\")\n",
    "\n",
    "print(\"\\n\ud83d\udcc1 Output Files:\")\n",
    "print(f\"  Excel (formatted): {excel_path}\")\n",
    "print(f\"  Google Drive: {drive_path}\")\n",
    "print(f\"  Models: {MODEL_SAVE_DIR}/\")\n",
    "print(f\"  Plots: {RESULTS_DIR}/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 Ready to download results!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Excel file\n",
    "from google.colab import files\n",
    "files.download(excel_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}